{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd23b673",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dee3a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # Operating system operations\n",
    "import shutil as st # Copy operations\n",
    "import numpy as np # Array and vector-matrix operations\n",
    "import random # Shuffle datas\n",
    "import pickle # Save and load history files\n",
    "import cv2 # Resize images\n",
    "import matplotlib.image as mping # Read RGB images\n",
    "import seaborn # Confusion matrix display\n",
    "import pandas as pd # Data reading\n",
    "import matplotlib.pyplot as plt # Visualition\n",
    "import skimage # Loading images\n",
    "import keras_tuner as kt # Hyperparameter optimization\n",
    "\n",
    "\n",
    "# Required packages for our ConvNet\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Activation, GlobalAveragePooling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications import VGG16, InceptionV3, ResNet50, ResNet152,  DenseNet201, MobileNetV2, EfficientNetB0, EfficientNetB7\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras import models, layers, regularizers\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.tensorflow import balanced_batch_generator\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score, balanced_accuracy_score,  precision_score, recall_score, f1_score, matthews_corrcoef\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "\n",
    "# Required packages for extracting files, progression bar etc.\n",
    "from tqdm.auto import tqdm\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "from pathlib import Path\n",
    "from shutil import copyfileobj\n",
    "from tqdm.utils import CallbackIOWrapper\n",
    "from os import fspath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd74b8a",
   "metadata": {},
   "source": [
    "### Downloading and extracting the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52a7e2d",
   "metadata": {},
   "source": [
    "The following section will download the necessarry dataset and the models. The 'Datas' folder contains:\n",
    " - Training set: Images for the training process (8512)\n",
    " - Validation set: Images for validation (751)\n",
    " - Test set: Images for testing (752)\n",
    " - Gaussian test set: Test set images with Gaussian noise\n",
    " - Poisson test set: Test set images with Poisson noise\n",
    " - test: Test images for check the CNN confidence\n",
    "\n",
    "The models folder contains:\n",
    " - default models: The default models. They have been trained with transfer learning (all layers frozen, feature extraction)\n",
    " - optimized models: The optimized models. They have been trained with transfer learning (some layers were newly trained and optimized)\n",
    " - ensemble models: The final model with ensemble technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf3f4a5",
   "metadata": {},
   "source": [
    "Helper function for creating directories, extracting files with progression bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5695cd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folders():\n",
    "    \n",
    "    models_dir                = 'new_models'\n",
    "    metrics_dir               = 'metrics'\n",
    "    metrics_visualization_dir = 'metrics_visualization'\n",
    "    plots_dir                 = 'plots'\n",
    "    heatmaps_dir              = 'heatmaps'\n",
    "    roc_auc_dir               = 'ROC-AUC'\n",
    "    hyperparameters_dir       = 'hyperparameters'\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(models_dir)\n",
    "        os.mkdir(os.path.join(models_dir, 'default'))\n",
    "        os.mkdir(os.path.join(models_dir, 'optimized'))\n",
    "        \n",
    "        os.mkdir(metrics_dir)\n",
    "        os.mkdir(os.path.join(metrics_dir, 'default'))\n",
    "        os.mkdir(os.path.join(metrics_dir, 'optimized'))\n",
    "        os.mkdir(os.path.join(metrics_dir, 'ensemble'))\n",
    "        os.mkdir(os.path.join(metrics_dir, 'poisson'))\n",
    "        os.mkdir(os.path.join(metrics_dir, 'gauss'))\n",
    "        os.mkdir(os.path.join(metrics_dir, 'FGSM'))\n",
    "\n",
    "        os.mkdir(metrics_visualization_dir)\n",
    "        os.mkdir(os.path.join(metrics_visualization_dir, 'default'))\n",
    "        os.mkdir(os.path.join(metrics_visualization_dir, 'optimized'))\n",
    "        os.mkdir(os.path.join(metrics_visualization_dir, 'ensemble'))\n",
    "\n",
    "        os.mkdir(plots_dir)\n",
    "        os.mkdir(os.path.join(plots_dir, 'default'))\n",
    "        os.mkdir(os.path.join(plots_dir, 'optimized'))\n",
    "        \n",
    "        os.mkdir(heatmaps_dir)\n",
    "        os.mkdir(os.path.join(heatmaps_dir, 'default'))\n",
    "        os.mkdir(os.path.join(heatmaps_dir, 'optimized'))\n",
    "        os.mkdir(os.path.join(heatmaps_dir, 'ensemble'))\n",
    "        os.mkdir(os.path.join(heatmaps_dir, 'perturbation'))\n",
    "\n",
    "        os.mkdir(roc_auc_dir)\n",
    "        os.mkdir(os.path.join(roc_auc_dir, 'default'))\n",
    "        os.mkdir(os.path.join(roc_auc_dir, 'optimized'))\n",
    "        os.mkdir(os.path.join(roc_auc_dir, 'ensemble'))\n",
    "        \n",
    "        os.mkdir(hyperparameters_dir)\n",
    "        \n",
    "        print('Directories were successfully created!')\n",
    "    except OSError as error:\n",
    "        print(error)    \n",
    "\n",
    "def extractall(fzip, dest, desc=\"Extracting\"):\n",
    "    \n",
    "    \"\"\"zipfile.Zipfile(fzip).extractall(dest) with progress\"\"\"\n",
    "    dest = Path(dest).expanduser()\n",
    "    with ZipFile(fzip) as zipf, tqdm(\n",
    "        desc=desc, unit=\"B\", unit_scale=True, unit_divisor=1024,\n",
    "        total=sum(getattr(i, \"file_size\", 0) for i in zipf.infolist()),\n",
    "    ) as pbar:\n",
    "        for i in zipf.infolist():\n",
    "            if not getattr(i, \"file_size\", 0):  # directory\n",
    "                zipf.extract(i, fspath(dest))\n",
    "            else:\n",
    "                with zipf.open(i) as fi, open(fspath(dest / i.filename), \"wb\") as fo:\n",
    "                    copyfileobj(CallbackIOWrapper(pbar.update, fi), fo)                   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72a742b",
   "metadata": {},
   "source": [
    "Downloading the datasets. If it does not work, please download the datasets directly from the following links and copy into the code folder:\n",
    " - https://drive.google.com/file/d/1gCMmE9Wjm7YXkwRNW5wS85tim4z_m-br/view?usp=sharing\n",
    " - https://drive.google.com/file/d/1_V3xN3MoqjLSdbjFDalFRhQm5R29nAG7/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e9a2645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Access denied with the following error:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\habon\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\gdown\\cli.py:127: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
      "  warnings.warn(\n",
      "\n",
      " \tToo many users have viewed or downloaded this file recently. Please\n",
      "\ttry accessing the file again later. If the file you are trying to\n",
      "\taccess is particularly large or is shared with many people, it may\n",
      "\ttake up to 24 hours to be able to view or download the file. If you\n",
      "\tstill can't access a file after 24 hours, contact your domain\n",
      "\tadministrator. \n",
      "\n",
      "You may still be able to access the file from the browser:\n",
      "\n",
      "\t https://drive.google.com/uc?id=1gCMmE9Wjm7YXkwRNW5wS85tim4z_m-br \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Access denied with the following error:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\habon\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\gdown\\cli.py:127: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
      "  warnings.warn(\n",
      "\n",
      " \tToo many users have viewed or downloaded this file recently. Please\n",
      "\ttry accessing the file again later. If the file you are trying to\n",
      "\taccess is particularly large or is shared with many people, it may\n",
      "\ttake up to 24 hours to be able to view or download the file. If you\n",
      "\tstill can't access a file after 24 hours, contact your domain\n",
      "\tadministrator. \n",
      "\n",
      "You may still be able to access the file from the browser:\n",
      "\n",
      "\t https://drive.google.com/uc?id=1_V3xN3MoqjLSdbjFDalFRhQm5R29nAG7 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#https://drive.google.com/file/d/1gCMmE9Wjm7YXkwRNW5wS85tim4z_m-br/view?usp=sharing\n",
    "#https://drive.google.com/file/d/1_V3xN3MoqjLSdbjFDalFRhQm5R29nAG7/view?usp=sharing\n",
    "\n",
    "!gdown --id 1gCMmE9Wjm7YXkwRNW5wS85tim4z_m-br\n",
    "!gdown --id 1_V3xN3MoqjLSdbjFDalFRhQm5R29nAG7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fb4fb1",
   "metadata": {},
   "source": [
    "Extracting the downloaded datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4740fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractall('Datas.zip', '.')\n",
    "extractall('models.zip', '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fed1e6",
   "metadata": {},
   "source": [
    "Creating directories for the models, metrics, heatmaps, plots etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb480b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_folders()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda42d0e",
   "metadata": {},
   "source": [
    "### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f85261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categories, Path and sizes\n",
    "\n",
    "multi_categories = ['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc'] # The multiclass images categories\n",
    "\n",
    "multi_train = './Datas/Multiclass/Training set'\n",
    "multi_val = './Datas/Multiclass/Validation set'\n",
    "multi_test = './Datas/Multiclass/Test set'\n",
    "\n",
    "multiclass_directories = [multi_train, multi_val, multi_test]\n",
    "\n",
    "multi_train_size = 8512\n",
    "multi_val_size   = 751\n",
    "multi_test_size  = 752\n",
    "\n",
    "multi_sizes = [multi_train_size, multi_val_size, multi_test_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e14441",
   "metadata": {},
   "source": [
    "### Class for balancing the dataset and for the CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97739ec7",
   "metadata": {},
   "source": [
    "DataBalancerGenerator class:\n",
    " - Creating a new, balanced dataset with data augmentation technique\n",
    "\n",
    "SkinCancerCnn class:\n",
    "\n",
    " - The class for the CNNs. This class has all the necessary functions for preprocessing, training, testing, metrics, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f0f5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#========================== Create a DataBalancerGenerator class, which help us to balancing the imbalanced dataset =========================#\n",
    "\n",
    "class DataBalancerGenerator(Sequence): # inherited from Sequence\n",
    "\n",
    "    # initialize the class members, and applying Data augmentation and Random oversampling\n",
    "    def __init__(self, x, y, datagen, batch_size = 32):\n",
    "    \n",
    "        self.datagen    = datagen # ImageDataGenerator, which we use to Data augmentation\n",
    "        self.batch_size = batch_size\n",
    "        self.shape      = x.shape        \n",
    "        \n",
    "        datagen.fit(x)# compute quantities required for featurewise normalization  (std, mean, and principal components if ZCA whitening is applied)\n",
    "        self.balanced_gen, self.steps_per_epoch = balanced_batch_generator(x.reshape(x.shape[0], -1),\n",
    "                                                                           y           = to_categorical(y, num_classes = 7),\n",
    "                                                                           sampler     = RandomOverSampler(),\n",
    "                                                                           batch_size  = self.batch_size,\n",
    "                                                                           keep_sparse = True) # balacing the dataset\n",
    "    \n",
    "    # determine the number of batches in the Sequence.\n",
    "    def __len__(self):\n",
    "        return self.steps_per_epoch\n",
    " \n",
    "\n",
    "    # create batches and use Data augmentation\n",
    "    def __getitem__(self, idx): # idx here is a mandatory parameter, but we don't use this.\n",
    "    \n",
    "        x_batch, y_batch = self.balanced_gen.__next__()\n",
    "        x_batch = x_batch.reshape(-1, *self.shape[1:])\n",
    "        \n",
    "        return self.datagen.flow(x_batch, y_batch, batch_size = self.batch_size).next() # .flow returns an iterator, so we have to use next() method to get the batches with tuple type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e4aace",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkinCancerCnn:\n",
    "\n",
    "    # initialize the class members\n",
    "    def __init__(self, image_size,multi_learning_rate, multi_batch_size, multi_decrease_lr, multi_early_stop, multi_epochs_num):\n",
    "    \n",
    "        self.image_size           = image_size           # the given image size (the size that we want to use  to train a model in our ConvNet)\n",
    "        self.multi_learning_rate  = multi_learning_rate  # the multiclass classification model learning rate\n",
    "        self.multi_batch_size     = multi_batch_size     # the multiclass classification model batch size (defines the number of samples that will be propagated through the network.)\n",
    "        self.multi_decrease_lr    = multi_decrease_lr    # the multiclass classification model decrease learning rate (when this is true, the learning rate will be decreasing)\n",
    "        self.multi_early_stop     = multi_early_stop     # the multiclass classification model early stopping (when this is true, the training will stop when the validation loss won't decrease)\n",
    "        self.multi_epochs_num     = multi_epochs_num     # the multiclass classification models epochs number (the number of complete passes through the training dataset)\n",
    "        \n",
    "    \n",
    "    # load the images and create a numpy image array, which contains all the images that we need\n",
    "    @staticmethod\n",
    "    def create_image_arr(img_size, path, datasets, preprocesser, mode):\n",
    "    \n",
    "        images = []\n",
    "\n",
    "        for dataset in datasets:\n",
    "            images_list = os.listdir(path + '/' +  dataset)\n",
    "            img_label   = datasets.index(dataset) # we need to map the labels to numbers(indexes 0-6)\n",
    "\n",
    "            for img in images_list:\n",
    "                my_image = mping.imread(path + '/' + dataset + '/' + img) # read the images\n",
    "                my_image = cv2.resize(my_image, (img_size, img_size),) # resize the images\n",
    "                images.append([my_image, img_label]) \n",
    "\n",
    "        if mode == 'train':\n",
    "            random.shuffle(images) # we have to shuffle the images, because if we don't, then the model start to memorize them\n",
    "\n",
    "        X = []\n",
    "        y = []\n",
    "\n",
    "        for matrix, label in images:\n",
    "            X.append(matrix)\n",
    "            y.append(label)\n",
    "\n",
    "        X = np.array(X) # we need to map the list into a numpy array\n",
    "        y = np.array(y) # the corresponding labels\n",
    "\n",
    "        if mode == 'test':\n",
    "            X = preprocesser(X) # Normalize data \n",
    "\n",
    "        print('Image array done with shape: ' + str(X.shape)) # the numpy array (matrix) shape\n",
    "\n",
    "        return X,y # return with the image array and the corresponding labels\n",
    "\n",
    "    \n",
    "    # Load the images with ImageDataGenerator which allow us to preprocess them easily\n",
    "    def create_multiclass_generators(self, X, y, train_path, val_path, multi_nums_arr, preprocesser):\n",
    "\n",
    "        # we use this ImageDataGenerator for Data augmentation (because the dataset is pretty imbalanced and we have to correct this)\n",
    "        datagen = ImageDataGenerator(preprocessing_function = preprocesser,\n",
    "                                     rotation_range         = 180,\n",
    "                                     width_shift_range      = 0.08,\n",
    "                                     height_shift_range     = 0.08,\n",
    "                                     zoom_range             = 0.065,\n",
    "                                     horizontal_flip        = True,\n",
    "                                     vertical_flip          = True,\n",
    "                                     fill_mode              = 'nearest') \n",
    "        \n",
    "        batch_size = self.multi_batch_size\n",
    "\n",
    "        balanced_gen = DataBalancerGenerator(X, y, datagen, batch_size = self.multi_batch_size) # Oversampling and Data augmentation to get more samples \n",
    "        \n",
    "        val_steps = multi_nums_arr[1] // batch_size\n",
    "\n",
    "        # we will use this generator under the training session in order to evaluate the model\n",
    "        val_datagen = ImageDataGenerator(preprocessing_function = preprocesser)\n",
    "\n",
    "        val_gen = val_datagen.flow_from_directory(val_path,\n",
    "                                                  target_size = (self.image_size,self.image_size),\n",
    "                                                  batch_size  = batch_size,\n",
    "                                                  class_mode  = \"categorical\") \n",
    "\n",
    "        return balanced_gen, val_gen, val_steps # return with the 2 generator (which basically an iterator type) and the corresponding steps (we will use that later in the model)\n",
    "    \n",
    "    \n",
    "    # Create multiclass classification model with transfer learning\n",
    "    def multi_class_model(self, train_gen, val_gen, val_steps, current_model, units, num_layers, dropout, globlayer, freeze_layers_num):\n",
    "             \n",
    "        # The pre-trained CNN\n",
    "        conv_base = current_model(weights     = 'imagenet', # initialize the weights\n",
    "                                  include_top = False, # we don't want to use the top layers ( we want to use our densely connected classifier)\n",
    "                                  input_shape = (self.image_size, self.image_size, 3)) # the input shape( usually it's (224,224,3))\n",
    "      \n",
    "        # Our dataset is different to any subset of the imagenet dataset, therefore freezing will mean a decrease in accuracy\n",
    "        # unfreezing some layers will allow us to optimize in the whole feature space, allowing to find better optima    \n",
    "\n",
    "        if freeze_layers_num == 0:\n",
    "            conv_base.trainable = False\n",
    "        else:\n",
    "            for layer in conv_base.layers[:freeze_layers_num]:\n",
    "                layer.trainable = False \n",
    "        \n",
    "        model = models.Sequential()\n",
    "        model.add(conv_base)\n",
    "        \n",
    "        if globlayer:\n",
    "            model.add(GlobalAveragePooling2D())\n",
    "\n",
    "        model.add(layers.Flatten()) # flattens the 3D tensor of embeddings into a 2D tensor of shape \n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            \n",
    "            model.add(layers.Dense(units, activation = 'relu', kernel_regularizer = regularizers.l2(0.01))) # use Regularization and Dropout in order to reduce overfitting\n",
    "            \n",
    "            if dropout: \n",
    "                model.add(layers.Dropout(0.35))\n",
    "                \n",
    "        model.add(layers.Dense(7, activation = 'softmax')) # final layer with 7 output        \n",
    "        \n",
    "        # compile and run the model\n",
    "        model.compile(loss      = 'categorical_crossentropy', # Multiclass, single-label classification\n",
    "                      optimizer = Adam(learning_rate = self.multi_learning_rate), # use RMSprop optimizer with the given learning rate\n",
    "                      metrics   = [\"accuracy\",\n",
    "                                   tf.keras.metrics.Precision(name = 'precision'),\n",
    "                                   tf.keras.metrics.Recall(name = 'recall')]) # we only care about accuracy \n",
    "\n",
    "        return model # return the model and the corresponding history (we will use these things later)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Create multiclass classification training process\n",
    "    def multi_class_model_train(self, train_gen, val_gen, val_steps, current_model, units, num_layers, dropout, globlayer, freeze_layers_num, model_name):\n",
    "    \n",
    "        model = self.multi_class_model(train_gen,\n",
    "                                       val_gen,\n",
    "                                       val_steps,\n",
    "                                       current_model,\n",
    "                                       units,\n",
    "                                       num_layers,\n",
    "                                       dropout,\n",
    "                                       globlayer,\n",
    "                                       freeze_layers_num)\n",
    "        \n",
    "        callbacks = []\n",
    "        \n",
    "        # For TensorBoard visualisation\n",
    "        #tensorboard = TensorBoard(log_dir = 'logs/{}'.format(model_name), histogram_freq = 1)\n",
    "        #callbacks.append(tensorboard)\n",
    "            \n",
    "        # If the user want to decrease the learning rate under the training session\n",
    "        if self.multi_decrease_lr:\n",
    "            decrease_lr = ReduceLROnPlateau(monitor   = 'val_recall',\n",
    "                                            factor    = 0.2,\n",
    "                                            min_lr    = 0.0000001,\n",
    "                                            patience  = 2,\n",
    "                                            verbose   = 1,\n",
    "                                            min_delta = 1e-6,\n",
    "                                            mode      = 'max')\n",
    "                                            \n",
    "            callbacks.append(decrease_lr)\n",
    "            \n",
    "        # If the user want to use early stopping under the training session  \n",
    "        if self.multi_early_stop:\n",
    "            early_stop = EarlyStopping(monitor              = 'val_recall',\n",
    "                                       min_delta            = 0,\n",
    "                                       patience             = 3,\n",
    "                                       verbose              = 0,\n",
    "                                       mode                 = 'max',\n",
    "                                       baseline             = None,\n",
    "                                       restore_best_weights = True)\n",
    "                                       \n",
    "            callbacks.append(early_stop)\n",
    "\n",
    "        history = model.fit(\n",
    "                    train_gen,\n",
    "                    callbacks        = callbacks,\n",
    "                    epochs           = self.multi_epochs_num,\n",
    "                    validation_data  = val_gen,\n",
    "                    validation_steps = val_steps)\n",
    "\n",
    "        return model, history # return the model and the corresponding history\n",
    "    \n",
    "    \n",
    "    # Calculate and save the metrics\n",
    "    @staticmethod\n",
    "    def metrics_scores(y_true, y_pred, filename):\n",
    "        # precision - The predictions are the baseline\n",
    "        # recall - The grand truth labels are the baseline\n",
    "        # f1-score - The harmonic mean of percision and recall (overall performance of the model)\n",
    "\n",
    "        accuracy          = accuracy_score(y_true, y_pred) # How many we got right\n",
    "        recall            = recall_score(y_true, y_pred, average = 'macro') # Truth labels are the baseline, All class[i] truth how many we got right\n",
    "        precision         = precision_score(y_true, y_pred, average = 'macro') # All class[i] predictions how many we got right\n",
    "        f1_score          = 2 * (precision * recall) / (precision + recall)\n",
    "        balanced_accuracy = balanced_accuracy_score(y_true, y_pred)\n",
    "        mcc               = matthews_corrcoef(y_true, y_pred)\n",
    "        avg               = (balanced_accuracy + f1_score + mcc) / 3.0\n",
    "        \n",
    "        format_accuracy          = \"{:.4f}\".format(accuracy)\n",
    "        format_precision         = \"{:.4f}\".format(precision)\n",
    "        format_balanced_accuracy = \"{:.4f}\".format(balanced_accuracy)\n",
    "        format_f1_score          = \"{:.4f}\".format(f1_score)\n",
    "        format_mcc               = \"{:.4f}\".format(mcc)\n",
    "        format_avg               = \"{:.4f}\".format(avg)\n",
    "        \n",
    "        acc_format = \"{:.2f}\".format(accuracy * 100)\n",
    "        avg_format = \"{:.2f}\".format(avg * 100)\n",
    "        \n",
    "        # Save the metrics\n",
    "        with open(filename, 'w') as file:\n",
    "\n",
    "            file.write(\"Accuracy of the total model is {:.4f}\".format(accuracy) + '\\n')\n",
    "            file.write(\"Precision of the total model is {:.4f}\".format(precision) + '\\n')\n",
    "            file.write(\"Recall of the total model is {:.4f}\".format(recall) + '\\n')\n",
    "            file.write(\"Balanced accuracy of the total model is {:.4f}\".format(balanced_accuracy) + '\\n')\n",
    "            file.write(\"F1 score of the total model is {:.4f}\".format(f1_score) + '\\n')\n",
    "            file.write(\"MCC of the total model is {:.4f}\".format(mcc) + '\\n')  \n",
    "            file.write(\"AVG of the total model is {:.4f}\".format(avg) + '\\n')  \n",
    "            file.write('\\n\\n\\n')\n",
    "            file.write(format_accuracy + '\\t' + format_precision + '\\t' + format_balanced_accuracy + '\\t' + format_f1_score + '\\t' + format_mcc + '\\t' + format_avg)\n",
    "            file.write('\\n\\n')\n",
    "            file.write(acc_format)\n",
    "            file.write('\\n')\n",
    "            file.write(avg_format)\n",
    "        \n",
    "        \n",
    "        print(classification_report(y_true, y_pred))\n",
    "        \n",
    "    \n",
    "    # Plot the confusion matrix   \n",
    "    @staticmethod\n",
    "    def confusion_matrix(y_true, y_pred, norm, filename):\n",
    "        \n",
    "        plt.rcParams[\"figure.figsize\"] = (6,4)\n",
    "\n",
    "        cm      = confusion_matrix(y_true = y_true, y_pred = y_pred, normalize = norm)\n",
    "        ax      = plt.subplot()\n",
    "        \n",
    "        if norm == None:\n",
    "            heatmap = seaborn.heatmap(cm, annot = True, cmap = \"Blues\", annot_kws = {\"size\" : 12}, fmt = 'd') \n",
    "        else:\n",
    "            heatmap = seaborn.heatmap(cm, annot = True, cmap = \"Blues\", annot_kws = {\"size\" : 12}) \n",
    "        \n",
    "        ax.set_xlabel('Predicted labels')\n",
    "        ax.set_ylabel('True labels')\n",
    "        ax.set_title('Confusion Matrix')\n",
    "        ax.xaxis.set_ticklabels(['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc'])\n",
    "        ax.yaxis.set_ticklabels(['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc'])\n",
    "        ax.figure.savefig(filename, dpi = 400)        \n",
    "        \n",
    "    \n",
    "    # Plot the training process \n",
    "    @staticmethod    \n",
    "    def plot_model(history, filename):\n",
    "        \n",
    "        accuracy      = history.history['accuracy']\n",
    "        val_accuracy  = history.history['val_accuracy']\n",
    "        \n",
    "        recall     = history.history['recall']\n",
    "        val_recall = history.history['val_recall']\n",
    "\n",
    "        loss     = history.history['loss']\n",
    "        val_loss = history.history['val_loss']\n",
    "\n",
    "        fig, ax    = plt.subplots(1, 3, figsize=(20, 8))\n",
    "        ax         = ax.ravel()\n",
    "        num_epochs = len(history.history['loss'])\n",
    "\n",
    "        for i, met in enumerate(['accuracy', 'recall', 'loss']):\n",
    "            ax[i].plot(history.history[met])\n",
    "            ax[i].plot(history.history['val_' + met])\n",
    "            ax[i].set_title('Model {}'.format(met))\n",
    "            ax[i].set_xlabel('epochs')\n",
    "            ax[i].set_xticks(range(1,num_epochs))\n",
    "            ax[i].set_ylabel(met)\n",
    "            ax[i].legend(['Training ' + met, 'Validation ' + met])\n",
    "\n",
    "        plt.savefig(filename)  \n",
    "        \n",
    "    \n",
    "    # Plot ROC-AUC curve\n",
    "    @staticmethod        \n",
    "    def plot_ROC_AUC_curve(y_true, y_pred, filename):\n",
    "\n",
    "        # roc curve for classes\n",
    "        fpr = {}\n",
    "        tpr = {}\n",
    "        thresh ={}\n",
    "        roc_auc = dict()\n",
    "\n",
    "        n_class = 7\n",
    "        plt.rcParams[\"figure.figsize\"] = (12,8)\n",
    "\n",
    "        y_t = label_binarize(y_true, classes = np.unique(y_true))\n",
    "\n",
    "        for i in range(n_class):    \n",
    "            fpr[i], tpr[i], thresh[i] = roc_curve(y_t[:,i], y_pred[:,i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "            # plotting    \n",
    "            plt.plot(fpr[i], tpr[i], linestyle='--', \n",
    "                     label='%s vs Rest (AUC=%0.2f)'%(SkinCancerCnn.map_labels(i),roc_auc[i]))\n",
    "\n",
    "        plt.plot([0,1],[0,1],'b--')\n",
    "        plt.xlim([0,1])\n",
    "        plt.ylim([0,1.05])\n",
    "        plt.title('Multiclass ROC curve')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive rate')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.savefig(filename)\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    # Map the indexes into labels\n",
    "    @staticmethod\n",
    "    def map_labels(index):\n",
    "\n",
    "        labels_dict={\n",
    "          0: 'Actinic Keratoses and Intraepithelial Carcinoma', # akiec\n",
    "          1: 'Basal cell carcinoma', # bcc\n",
    "          2: 'Benign keratosis-like lesions', # bkl\n",
    "          3: 'Dermatofibroma', # df\n",
    "          4: 'Melanoma', # mel \n",
    "          5: 'Melanocytic nevi', # nv\n",
    "          6: 'Vascular lesions' # vasc\n",
    "        } \n",
    "\n",
    "        return labels_dict[index] # return with the right value\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfb079a",
   "metadata": {},
   "source": [
    "### Default models training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f653754a",
   "metadata": {},
   "source": [
    "In this section 8 different models have been trained with the following parameters:\n",
    " - Optimizer: Adam\n",
    " - Learning rate: initial learning rate is 0.0001\n",
    " - Batch size: 64\n",
    " - Max epochs num: 15\n",
    " - ReduceLROnPlateau with patience 1\n",
    " - EarlyStopping with patience 2\n",
    " \n",
    "The only change in the models architecture is the final fully connected/Dense layer, which contains 7 output instead of 1000. During the training process all layers were frozen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6919c911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for training \n",
    "lr                = 0.0001\n",
    "batch_size        = 64\n",
    "img_size          = 224\n",
    "epochs_num        = 15\n",
    "reduce_lr         = True\n",
    "early_stop        = True\n",
    "my_skin_cnn       = SkinCancerCnn(img_size, lr, batch_size, reduce_lr, early_stop, epochs_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31938a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG16\n",
    "\n",
    "current_model     = VGG16\n",
    "units             = 0\n",
    "num_layers        = 0\n",
    "dropout           = False\n",
    "globlayer         = False\n",
    "freeze_layers_num = 0\n",
    "model_name        = 'VGG16'\n",
    "\n",
    "# Create numpy array and generators for the model\n",
    "X_train, y_train = SkinCancerCnn.create_image_arr(img_size, multiclass_directories[0],\n",
    "                                                  multi_categories,\n",
    "                                                  tf.keras.applications.vgg16.preprocess_input,\n",
    "                                                  'train')\n",
    "\n",
    "train_multi_generator, val_multi_generator, multi_val_steps = my_skin_cnn.create_multiclass_generators(X_train,\n",
    "                                                                                                       y_train,\n",
    "                                                                                                       multiclass_directories[0],\n",
    "                                                                                                       multiclass_directories[1],                                       \n",
    "                                                                                                       multi_sizes,\n",
    "                                                                                                       tf.keras.applications.vgg16.preprocess_input)\n",
    "\n",
    "# Training process\n",
    "vgg16_model, vgg16_history = my_skin_cnn.multi_class_model_train(train_multi_generator,\n",
    "                                                                 val_multi_generator,\n",
    "                                                                 multi_val_steps, \n",
    "                                                                 current_model,\n",
    "                                                                 units,\n",
    "                                                                 num_layers,\n",
    "                                                                 dropout,\n",
    "                                                                 globlayer,\n",
    "                                                                 freeze_layers_num,\n",
    "                                                                 model_name)\n",
    "# Plot the training process\n",
    "SkinCancerCnn.plot_model(vgg16_history, 'plots/default/VGG16_default_plot.jpg')\n",
    "\n",
    "# Save the model\n",
    "vgg16_model.save('./new_models/default/VGG16_224x224_batch64_frezze_full_original_0.0001lr.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56225084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# InceptionV3\n",
    "current_model     = InceptionV3\n",
    "units             = 0\n",
    "num_layers        = 0\n",
    "dropout           = False\n",
    "globlayer         = False\n",
    "freeze_layers_num = 0\n",
    "model_name        = 'InceptionV3'\n",
    "\n",
    "# Create numpy array and generators for the model\n",
    "X_train, y_train = SkinCancerCnn.create_image_arr(img_size,\n",
    "                                                  multiclass_directories[0],\n",
    "                                                  multi_categories,  tf.keras.applications.inception_v3.preprocess_input,\n",
    "                                                  'train')\n",
    "\n",
    "train_multi_generator, val_multi_generator, multi_val_steps = my_skin_cnn.create_multiclass_generators(X_train,\n",
    "                                                                                                       y_train,\n",
    "                                                                                                       multiclass_directories[0],\n",
    "                                                                                                       multiclass_directories[1],                                       \n",
    "                                                                                                       multi_sizes,\n",
    "                                                                                                       tf.keras.applications.inception_v3.preprocess_input)\n",
    "# Training process\n",
    "inceptionv3_model, inceptionv3_history = my_skin_cnn.multi_class_model_train(train_multi_generator,\n",
    "                                                                             val_multi_generator,\n",
    "                                                                             multi_val_steps, \n",
    "                                                                             current_model,\n",
    "                                                                             units,\n",
    "                                                                             num_layers,\n",
    "                                                                             dropout,\n",
    "                                                                             globlayer,\n",
    "                                                                             freeze_layers_num,\n",
    "                                                                             model_name)\n",
    "# Plot the training process\n",
    "SkinCancerCnn.plot_model(inceptionv3_history, 'plots/default/InceptionV3_default_plot.jpg')\n",
    "\n",
    "# Save the model\n",
    "inceptionv3_model.save('./new_models/default/InceptionV3_224x224_batch64_frezze_full_original_0.0001lr.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09cbebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet50\n",
    "current_model     = ResNet50\n",
    "units             = 0\n",
    "num_layers        = 0\n",
    "dropout           = False\n",
    "globlayer         = False\n",
    "freeze_layers_num = 0\n",
    "model_name        = 'ResNet50'\n",
    "\n",
    "# Create numpy array and generators for the model\n",
    "X_train, y_train = SkinCancerCnn.create_image_arr(img_size,\n",
    "                                                  multiclass_directories[0],\n",
    "                                                  multi_categories,\n",
    "                                                  tf.keras.applications.resnet.preprocess_input,\n",
    "                                                  'train')\n",
    "\n",
    "train_multi_generator, val_multi_generator, multi_val_steps = my_skin_cnn.create_multiclass_generators(X_train,\n",
    "                                                                                                       y_train,\n",
    "                                                                                                       multiclass_directories[0],\n",
    "                                                                                                       multiclass_directories[1],                                       \n",
    "                                                                                                       multi_sizes,\n",
    "                                                                                                       tf.keras.applications.resnet.preprocess_input)\n",
    "\n",
    "# Training process\n",
    "resnet50_model, resnet50_history = my_skin_cnn.multi_class_model_train(train_multi_generator,\n",
    "                                                                       val_multi_generator,\n",
    "                                                                       multi_val_steps, \n",
    "                                                                       current_model,\n",
    "                                                                       units,\n",
    "                                                                       num_layers,\n",
    "                                                                       dropout,\n",
    "                                                                       globlayer,\n",
    "                                                                       freeze_layers_num,\n",
    "                                                                       model_name)\n",
    "# Plot the training process\n",
    "SkinCancerCnn.plot_model(resnet50_history, 'plots/default/ResNet50_default_plot.jpg')\n",
    "\n",
    "# Save the model\n",
    "resnet50_model.save('./new_models/default/ResNet50_224x224_batch64_frezze_full_original_0.0001lr.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03a1a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet152\n",
    "current_model     = ResNet152\n",
    "units             = 0\n",
    "num_layers        = 0\n",
    "dropout           = False\n",
    "globlayer         = False\n",
    "freeze_layers_num = 0\n",
    "model_name        = 'ResNet152'\n",
    "\n",
    "# Create numpy array and generators for the model\n",
    "X_train, y_train = SkinCancerCnn.create_image_arr(img_size, \n",
    "                                                  multiclass_directories[0],\n",
    "                                                  multi_categories,\n",
    "                                                  tf.keras.applications.resnet.preprocess_input,\n",
    "                                                  'train')\n",
    "\n",
    "train_multi_generator, val_multi_generator, multi_val_steps = my_skin_cnn.create_multiclass_generators(X_train,\n",
    "                                                                                                       y_train,\n",
    "                                                                                                       multiclass_directories[0],\n",
    "                                                                                                       multiclass_directories[1],                                       \n",
    "                                                                                                       multi_sizes,\n",
    "                                                                                                       tf.keras.applications.resnet.preprocess_input)\n",
    "\n",
    "# Training process\n",
    "resnet152_model, resnet152_history = my_skin_cnn.multi_class_model_train(train_multi_generator,\n",
    "                                                                         val_multi_generator,\n",
    "                                                                         multi_val_steps, \n",
    "                                                                         current_model,\n",
    "                                                                         units,\n",
    "                                                                         num_layers,\n",
    "                                                                         dropout,\n",
    "                                                                         globlayer,\n",
    "                                                                         freeze_layers_num,\n",
    "                                                                         model_name)\n",
    "# Plot the training process\n",
    "SkinCancerCnn.plot_model(resnet152_history, 'plots/default/ResNet152_default_plot.jpg')\n",
    "\n",
    "# Save the model\n",
    "resnet152_model.save('./new_models/default/ResNet152_224x224_batch64_frezze_full_original_0.0001lr.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188dd864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DenseNet201\n",
    "current_model     = DenseNet201\n",
    "units             = 0\n",
    "num_layers        = 0\n",
    "dropout           = False\n",
    "globlayer         = False\n",
    "freeze_layers_num = 0\n",
    "model_name        = 'DenseNet201'\n",
    "\n",
    "# Create numpy array and generators for the model\n",
    "X_train, y_train = SkinCancerCnn.create_image_arr(img_size,\n",
    "                                                  multiclass_directories[0],\n",
    "                                                  multi_categories,\n",
    "                                                  tf.keras.applications.densenet.preprocess_input,\n",
    "                                                  'train')\n",
    "\n",
    "train_multi_generator, val_multi_generator, multi_val_steps = my_skin_cnn.create_multiclass_generators(X_train,\n",
    "                                                                                                       y_train,\n",
    "                                                                                                       multiclass_directories[0],\n",
    "                                                                                                       multiclass_directories[1],                                       \n",
    "                                                                                                       multi_sizes,\n",
    "                                                                                                       tf.keras.applications.densenet.preprocess_input)\n",
    "\n",
    "# Training process\n",
    "densenet201_model, densenet201_history = my_skin_cnn.multi_class_model_train(train_multi_generator,\n",
    "                                                                             val_multi_generator,\n",
    "                                                                             multi_val_steps, \n",
    "                                                                             current_model,\n",
    "                                                                             units,\n",
    "                                                                             num_layers,\n",
    "                                                                             dropout,\n",
    "                                                                             globlayer,\n",
    "                                                                             freeze_layers_num,\n",
    "                                                                             model_name)\n",
    "\n",
    "# Plot the training process\n",
    "SkinCancerCnn.plot_model(densenet201_history, 'plots/default/DenseNet201_default_plot.jpg')\n",
    "\n",
    "# Save the model\n",
    "densenet201_model.save('./new_models/default/DenseNet201_224x224_batch64_frezze_full_original_0.0001lr.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c691509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MobileNetV2\n",
    "current_model     = MobileNetV2\n",
    "units             = 0\n",
    "num_layers        = 0\n",
    "dropout           = False\n",
    "globlayer         = False\n",
    "freeze_layers_num = 0\n",
    "model_name        = 'MobileNetV2'\n",
    "\n",
    "# Create numpy array and generators for the model\n",
    "X_train, y_train = SkinCancerCnn.create_image_arr(img_size,\n",
    "                                                  multiclass_directories[0],\n",
    "                                                  multi_categories,\n",
    "                                                  tf.keras.applications.mobilenet_v2.preprocess_input,\n",
    "                                                  'train')\n",
    "\n",
    "train_multi_generator, val_multi_generator, multi_val_steps = my_skin_cnn.create_multiclass_generators(X_train,\n",
    "                                                                                                       y_train,\n",
    "                                                                                                       multiclass_directories[0],\n",
    "                                                                                                       multiclass_directories[1],                                       \n",
    "                                                                                                       multi_sizes,\n",
    "                                                                                                       tf.keras.applications.mobilenet_v2.preprocess_input)\n",
    "\n",
    "# Training process\n",
    "mobilenetv2_model, mobilenetv2_history = my_skin_cnn.multi_class_model_train(train_multi_generator,\n",
    "                                                                             val_multi_generator,\n",
    "                                                                             multi_val_steps, \n",
    "                                                                             current_model,\n",
    "                                                                             units,\n",
    "                                                                             num_layers,\n",
    "                                                                             dropout,\n",
    "                                                                             globlayer,\n",
    "                                                                             freeze_layers_num,\n",
    "                                                                             model_name)\n",
    "\n",
    "# Plot the training process\n",
    "SkinCancerCnn.plot_model(mobilenetv2_history, 'plots/default/MobileNetV2_default_plot.jpg')\n",
    "\n",
    "# Save the model\n",
    "mobilenetv2_model.save('./new_models/default/MobileNetV2_224x224_batch64_frezze_full_original_0.0001lr.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2b9de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EfficientNetB0\n",
    "current_model     = EfficientNetB0\n",
    "units             = 0\n",
    "num_layers        = 0\n",
    "dropout           = False\n",
    "globlayer         = False\n",
    "freeze_layers_num = 0\n",
    "model_name        = 'EfficientNetB0'\n",
    "\n",
    "# Create numpy array and generators for the model\n",
    "X_train, y_train = SkinCancerCnn.create_image_arr(img_size,\n",
    "                                                  multiclass_directories[0],\n",
    "                                                  multi_categories,\n",
    "                                                  tf.keras.applications.efficientnet.preprocess_input,\n",
    "                                                  'train')\n",
    "\n",
    "train_multi_generator, val_multi_generator, multi_val_steps = my_skin_cnn.create_multiclass_generators(X_train,\n",
    "                                                                                                       y_train,\n",
    "                                                                                                       multiclass_directories[0],\n",
    "                                                                                                       multiclass_directories[1],                                       \n",
    "                                                                                                       multi_sizes,\n",
    "                                                                                                       tf.keras.applications.efficientnet.preprocess_input)\n",
    "\n",
    "# Training process\n",
    "efficientnetb0_model, efficientnetb0_history = my_skin_cnn.multi_class_model_train(train_multi_generator,\n",
    "                                                                             val_multi_generator,\n",
    "                                                                             multi_val_steps, \n",
    "                                                                             current_model,\n",
    "                                                                             units,\n",
    "                                                                             num_layers,\n",
    "                                                                             dropout,\n",
    "                                                                             globlayer,\n",
    "                                                                             freeze_layers_num,\n",
    "                                                                             model_name)\n",
    "\n",
    "# Plot the training process\n",
    "SkinCancerCnn.plot_model(efficientnetb0_history, 'plots/default/EfficientNetB0_default_plot.jpg')\n",
    "\n",
    "# Save the model\n",
    "efficientnetb0_model.save('./new_models/default/EfficientNetB0_224x224_batch64_frezze_full_original_0.0001lr.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea5f337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EfficientNetB7\n",
    "current_model     = EfficientNetB7\n",
    "units             = 0\n",
    "num_layers        = 0\n",
    "dropout           = False\n",
    "globlayer         = False\n",
    "freeze_layers_num = 0\n",
    "model_name        = 'EfficientNetB7'\n",
    "\n",
    "# Create numpy array and generators for the model\n",
    "X_train, y_train = SkinCancerCnn.create_image_arr(img_size,\n",
    "                                                  multiclass_directories[0], \n",
    "                                                  multi_categories,\n",
    "                                                  tf.keras.applications.efficientnet.preprocess_input,\n",
    "                                                  'train')\n",
    "\n",
    "train_multi_generator, val_multi_generator, multi_val_steps = my_skin_cnn.create_multiclass_generators(X_train,\n",
    "                                                                                                       y_train,\n",
    "                                                                                                       multiclass_directories[0],\n",
    "                                                                                                       multiclass_directories[1],                                       \n",
    "                                                                                                       multi_sizes,\n",
    "                                                                                                       tf.keras.applications.efficientnet.preprocess_input)\n",
    "\n",
    "# Training process\n",
    "efficientnetb7_model, efficientnetb7_history = my_skin_cnn.multi_class_model_train(train_multi_generator,\n",
    "                                                                                   val_multi_generator,\n",
    "                                                                                   multi_val_steps, \n",
    "                                                                                   current_model,\n",
    "                                                                                   units,\n",
    "                                                                                   num_layers,\n",
    "                                                                                   dropout,\n",
    "                                                                                   globlayer,\n",
    "                                                                                   freeze_layers_num,\n",
    "                                                                                   model_name)\n",
    "\n",
    "# Plot the training process\n",
    "SkinCancerCnn.plot_model(efficientnetb7_history, 'plots/default/EfficientNetB7_default_plot.jpg')\n",
    "\n",
    "# Save the model\n",
    "efficientnetb7_model.save('./new_models/default/EfficientNetB7_224x224_batch64_frezze_full_original_0.0001lr.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91975e69",
   "metadata": {},
   "source": [
    "### Optimized models training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ad00f3",
   "metadata": {},
   "source": [
    "In this section 8 different models have been trained with the following parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774999a0",
   "metadata": {},
   "source": [
    "|Model         |Number of new layers|Number of new neurons per layers| Dropout|Global average pooling|Number of frozen layers|\n",
    "|--------------|--------------------|--------------------------------|--------|----------------------|-----------------------|\n",
    "|VGG16         |1                   |512                             |False   |True                  |14                     |\n",
    "|InceptionV3   |3                   |256                             |False   |True                  |50                     |\n",
    "|ResNet50      |2                   |384                             |False   |False                 |90                     |\n",
    "|ResNet152     |3                   |128                             |False   |False                 |200                    |\n",
    "|DenseNet201   |2                   |384                             |True    |False                 |200                    |\n",
    "|MobileNetV2   |1                   |256                             |False   |False                 |35                     |\n",
    "|EfficientNetB0|3                   |384                             |False   |False                 |100                    |\n",
    "|EfficientNetB7|3                   |128                             |True    |True                  |400                    |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8bd6a0",
   "metadata": {},
   "source": [
    "The training process parameters are the followings:\n",
    " - Optimizer: Adam\n",
    " - Learning rate: 0.00001 initial\n",
    " - Batch size: 64\n",
    " - Max epochs number: 20\n",
    " - ReduceLROnPlateau with patience 2\n",
    " - EarlyStopping with patience 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0f609c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for training \n",
    "lr                = 0.00001\n",
    "batch_size        = 64\n",
    "img_size          = 224\n",
    "epochs_num        = 20\n",
    "reduce_lr         = True\n",
    "early_stop        = True\n",
    "my_skin_cnn_opt   = SkinCancerCnn(img_size, lr, batch_size, reduce_lr, early_stop, epochs_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9db6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG16 optimized \n",
    "\n",
    "current_model     = VGG16\n",
    "units             = 512\n",
    "num_layers        = 1\n",
    "dropout           = False\n",
    "globlayer         = True\n",
    "freeze_layers_num = 14\n",
    "model_name        = 'VGG16_optimized'\n",
    "\n",
    "# Create numpy array and generators for the model\n",
    "X_train, y_train = SkinCancerCnn.create_image_arr(img_size,\n",
    "                                                  multiclass_directories[0],\n",
    "                                                  multi_categories,\n",
    "                                                  tf.keras.applications.vgg16.preprocess_input,\n",
    "                                                  'train')\n",
    "\n",
    "train_multi_generator, val_multi_generator, multi_val_steps = my_skin_cnn_opt.create_multiclass_generators(X_train,\n",
    "                                                                                                           y_train,\n",
    "                                                                                                           multiclass_directories[0],\n",
    "                                                                                                           multiclass_directories[1],                                       \n",
    "                                                                                                           multi_sizes,\n",
    "                                                                                                           tf.keras.applications.vgg16.preprocess_input)\n",
    "\n",
    "# Training process\n",
    "vgg16_optimized_model, vgg16_optimized_history = my_skin_cnn_opt.multi_class_model_train(train_multi_generator,\n",
    "                                                                                         val_multi_generator,\n",
    "                                                                                         multi_val_steps, \n",
    "                                                                                         current_model,\n",
    "                                                                                         units,\n",
    "                                                                                         num_layers,\n",
    "                                                                                         dropout,\n",
    "                                                                                         globlayer,\n",
    "                                                                                         freeze_layers_num,\n",
    "                                                                                         model_name)\n",
    "\n",
    "# Plot the training process\n",
    "SkinCancerCnn.plot_model(vgg16_optimized_history, 'plots/optimized/VGG16_optimized_plot.jpg')\n",
    "\n",
    "# Save the model\n",
    "vgg16_optimized_model.save('./new_models/optimized/VGG16_optimized_224x224_batch64_0.00001lr.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974f1020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# InceptionV3 optimized\n",
    "\n",
    "current_model     = InceptionV3\n",
    "units             = 256\n",
    "num_layers        = 3\n",
    "dropout           = False\n",
    "globlayer         = True\n",
    "freeze_layers_num = 50\n",
    "model_name        = 'InceptionV3_optimized'\n",
    "\n",
    "# Create numpy array and generators for the model\n",
    "X_train, y_train = SkinCancerCnn.create_image_arr(img_size,\n",
    "                                                  multiclass_directories[0],\n",
    "                                                  multi_categories,\n",
    "                                                  tf.keras.applications.inception_v3.preprocess_input,\n",
    "                                                  'train')\n",
    "\n",
    "train_multi_generator, val_multi_generator, multi_val_steps = my_skin_cnn_opt.create_multiclass_generators(X_train,\n",
    "                                                                                                           y_train,\n",
    "                                                                                                           multiclass_directories[0],\n",
    "                                                                                                           multiclass_directories[1],                                       \n",
    "                                                                                                           multi_sizes,\n",
    "                                                                                                           tf.keras.applications.inception_v3.preprocess_input)\n",
    "\n",
    "# Training process\n",
    "inceptionv3_optimized_model, inceptionv3_optimized_history = my_skin_cnn_opt.multi_class_model_train(train_multi_generator,\n",
    "                                                                                                     val_multi_generator,\n",
    "                                                                                                     multi_val_steps, \n",
    "                                                                                                     current_model,\n",
    "                                                                                                     units,\n",
    "                                                                                                     num_layers,\n",
    "                                                                                                     dropout,\n",
    "                                                                                                     globlayer,\n",
    "                                                                                                     freeze_layers_num,\n",
    "                                                                                                     model_name)\n",
    "\n",
    "# Plot the training process\n",
    "SkinCancerCnn.plot_model(inceptionv3_optimized_history, 'plots/optimized/InceptionV3_optimized_plot.jpg')\n",
    "\n",
    "# Save the model\n",
    "inceptionv3_optimized_model.save('./new_models/optimized/InceptionV3_optimized_224x224_batch64_0.00001lr.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b358cd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet50 optimized \n",
    "\n",
    "current_model     = ResNet50\n",
    "units             = 384\n",
    "num_layers        = 2\n",
    "dropout           = False\n",
    "globlayer         = False\n",
    "freeze_layers_num = 90\n",
    "model_name        = 'ResNet50_optimized'\n",
    "\n",
    "# Create numpy array and generators for the model\n",
    "X_train, y_train = SkinCancerCnn.create_image_arr(img_size,\n",
    "                                                  multiclass_directories[0],\n",
    "                                                  multi_categories,\n",
    "                                                  tf.keras.applications.resnet.preprocess_input,\n",
    "                                                  'train')\n",
    "\n",
    "train_multi_generator, val_multi_generator, multi_val_steps = my_skin_cnn_opt.create_multiclass_generators(X_train,\n",
    "                                                                                                           y_train,\n",
    "                                                                                                           multiclass_directories[0],\n",
    "                                                                                                           multiclass_directories[1],                                       \n",
    "                                                                                                           multi_sizes,\n",
    "                                                                                                           tf.keras.applications.resnet.preprocess_input)\n",
    "\n",
    "# Training process\n",
    "resnet50_optimized_model, resnet50_optimized_history = my_skin_cnn_opt.multi_class_model_train(train_multi_generator,\n",
    "                                                                                               val_multi_generator,\n",
    "                                                                                               multi_val_steps, \n",
    "                                                                                               current_model,\n",
    "                                                                                               units,\n",
    "                                                                                               num_layers,\n",
    "                                                                                               dropout,\n",
    "                                                                                               globlayer,\n",
    "                                                                                               freeze_layers_num,\n",
    "                                                                                               model_name)\n",
    "\n",
    "# Plot the training process\n",
    "SkinCancerCnn.plot_model(resnet50_optimized_history, 'plots/optimized/ResNet50_optimized_plot.jpg')\n",
    "\n",
    "# Save the model\n",
    "resnet50_optimized_model.save('./new_models/optimized/ResNet50_optimized_224x224_batch64_0.00001lr.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b045a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet152 optimized\n",
    "\n",
    "current_model     = ResNet152\n",
    "units             = 128\n",
    "num_layers        = 3\n",
    "dropout           = False\n",
    "globlayer         = False\n",
    "freeze_layers_num = 200\n",
    "model_name        = 'ResNet152_optimized'\n",
    "\n",
    "# Create numpy array and generators for the model\n",
    "X_train, y_train = SkinCancerCnn.create_image_arr(img_size,\n",
    "                                                  multiclass_directories[0],\n",
    "                                                  multi_categories,\n",
    "                                                  tf.keras.applications.resnet.preprocess_input,\n",
    "                                                  'train')\n",
    "\n",
    "train_multi_generator, val_multi_generator, multi_val_steps = my_skin_cnn_opt.create_multiclass_generators(X_train,\n",
    "                                                                                                           y_train,\n",
    "                                                                                                           multiclass_directories[0],\n",
    "                                                                                                           multiclass_directories[1],                                       \n",
    "                                                                                                           multi_sizes,\n",
    "                                                                                                           tf.keras.applications.resnet.preprocess_input)\n",
    "\n",
    "# Training process\n",
    "resnet152_optimized_model, resnet152_optimized_history = my_skin_cnn_opt.multi_class_model_train(train_multi_generator,\n",
    "                                                                                                 val_multi_generator,\n",
    "                                                                                                 multi_val_steps, \n",
    "                                                                                                 current_model,\n",
    "                                                                                                 units,\n",
    "                                                                                                 num_layers,\n",
    "                                                                                                 dropout,\n",
    "                                                                                                 globlayer,\n",
    "                                                                                                 freeze_layers_num,\n",
    "                                                                                                 model_name)\n",
    "\n",
    "# Plot the training process\n",
    "SkinCancerCnn.plot_model(resnet152_optimized_history, 'plots/optimized/ResNet152_optimized_plot.jpg')\n",
    "\n",
    "# Save the model\n",
    "resnet152_optimized_model.save('./new_models/optimized/ResNet152_optimized_224x224_batch64_0.00001lr.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52911870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DenseNet201 optimized\n",
    "\n",
    "current_model     = DenseNet201\n",
    "units             = 384\n",
    "num_layers        = 2\n",
    "dropout           = True\n",
    "globlayer         = False\n",
    "freeze_layers_num = 200\n",
    "model_name        = 'DenseNet201_optimized'\n",
    "\n",
    "# Create numpy array and generators for the model\n",
    "X_train, y_train = SkinCancerCnn.create_image_arr(img_size,\n",
    "                                                  multiclass_directories[0],\n",
    "                                                  multi_categories,\n",
    "                                                  tf.keras.applications.densenet.preprocess_input,\n",
    "                                                  'train')\n",
    "\n",
    "train_multi_generator, val_multi_generator, multi_val_steps = my_skin_cnn_opt.create_multiclass_generators(X_train,\n",
    "                                                                                                           y_train,\n",
    "                                                                                                           multiclass_directories[0],\n",
    "                                                                                                           multiclass_directories[1],                                       \n",
    "                                                                                                           multi_sizes,\n",
    "                                                                                                           tf.keras.applications.densenet.preprocess_input)\n",
    "\n",
    "# Training process\n",
    "densenet201_optimized_model, densenet201_optimized_history = my_skin_cnn_opt.multi_class_model_train(train_multi_generator,\n",
    "                                                                                                     val_multi_generator,\n",
    "                                                                                                     multi_val_steps, \n",
    "                                                                                                     current_model,\n",
    "                                                                                                     units,\n",
    "                                                                                                     num_layers,\n",
    "                                                                                                     dropout,\n",
    "                                                                                                     globlayer,\n",
    "                                                                                                     freeze_layers_num,\n",
    "                                                                                                     model_name)\n",
    "\n",
    "# Plot the training process\n",
    "SkinCancerCnn.plot_model(densenet201_optimized_history, 'plots/optimized/DenseNet201_optimized_plot.jpg')\n",
    "\n",
    "# Save the model\n",
    "densenet201_optimized_model.save('./new_models/optimized/DenseNet201_optimized_224x224_batch64_0.00001lr.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9858a913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MobileNetV2 optimized\n",
    "\n",
    "current_model     = MobileNetV2\n",
    "units             = 256\n",
    "num_layers        = 1\n",
    "dropout           = False\n",
    "globlayer         = False\n",
    "freeze_layers_num = 35\n",
    "model_name        = 'MobileNetV2_optimized'\n",
    "\n",
    "# Create numpy array and generators for the model\n",
    "X_train, y_train = SkinCancerCnn.create_image_arr(img_size,\n",
    "                                                  multiclass_directories[0], \n",
    "                                                  multi_categories,\n",
    "                                                  tf.keras.applications.mobilenet_v2.preprocess_input,\n",
    "                                                  'train')\n",
    "\n",
    "train_multi_generator, val_multi_generator, multi_val_steps = my_skin_cnn_opt.create_multiclass_generators(X_train,\n",
    "                                                                                                           y_train,\n",
    "                                                                                                           multiclass_directories[0],\n",
    "                                                                                                           multiclass_directories[1],                                       \n",
    "                                                                                                           multi_sizes,\n",
    "                                                                                                           tf.keras.applications.mobilenet_v2.preprocess_input)\n",
    "\n",
    "# Training process\n",
    "mobilenetv2_optimized_model, mobilenetv2_optimized_history = my_skin_cnn_opt.multi_class_model_train(train_multi_generator,\n",
    "                                                                                                     val_multi_generator,\n",
    "                                                                                                     multi_val_steps, \n",
    "                                                                                                     current_model,\n",
    "                                                                                                     units,\n",
    "                                                                                                     num_layers,\n",
    "                                                                                                     dropout,\n",
    "                                                                                                     globlayer,\n",
    "                                                                                                     freeze_layers_num,\n",
    "                                                                                                     model_name)\n",
    "\n",
    "# Plot the training process\n",
    "SkinCancerCnn.plot_model(mobilenetv2_optimized_history, 'plots/optimized/MobileNetV2_optimized_plot.jpg')\n",
    "\n",
    "# Save the model\n",
    "mobilenetv2_optimized_model.save('./new_models/optimized/MobileNetV2_optimized_224x224_batch64_0.00001lr.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faea459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EfficientNetB0 optimized\n",
    "\n",
    "current_model     = EfficientNetB0\n",
    "units             = 384\n",
    "num_layers        = 3\n",
    "dropout           = False\n",
    "globlayer         = False\n",
    "freeze_layers_num = 100\n",
    "model_name        = 'EfficientNetB0_optimized'\n",
    "\n",
    "# Create numpy array and generators for the model\n",
    "X_train, y_train = SkinCancerCnn.create_image_arr(img_size, \n",
    "                                                  multiclass_directories[0], \n",
    "                                                  multi_categories,\n",
    "                                                  tf.keras.applications.efficientnet.preprocess_input,\n",
    "                                                  'train')\n",
    "\n",
    "train_multi_generator, val_multi_generator, multi_val_steps = my_skin_cnn_opt.create_multiclass_generators(X_train,\n",
    "                                                                                                           y_train,\n",
    "                                                                                                           multiclass_directories[0],\n",
    "                                                                                                           multiclass_directories[1],                                       \n",
    "                                                                                                           multi_sizes,\n",
    "                                                                                                           tf.keras.applications.efficientnet.preprocess_input)\n",
    "\n",
    "# Training process\n",
    "efficientnetb0_optimized_model, efficientnetb0_optimized_history = my_skin_cnn_opt.multi_class_model_train(train_multi_generator,\n",
    "                                                                                                           val_multi_generator,\n",
    "                                                                                                           multi_val_steps, \n",
    "                                                                                                           current_model,\n",
    "                                                                                                           units,\n",
    "                                                                                                           num_layers,\n",
    "                                                                                                           dropout,\n",
    "                                                                                                           globlayer,\n",
    "                                                                                                           freeze_layers_num,\n",
    "                                                                                                           model_name)\n",
    "\n",
    "# Plot the training process\n",
    "SkinCancerCnn.plot_model(efficientnetb0_optimized_history, 'plots/optimized/EfficientNetB0_optimized_plot.jpg')\n",
    "\n",
    "# Save the model\n",
    "efficientnetb0_optimized_model.save('./new_models/optimized/EfficientNetB0_optimized_224x224_batch64_0.00001lr.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa027f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EfficientNetB7 optimized\n",
    "\n",
    "current_model     = EfficientNetB7\n",
    "units             = 128\n",
    "num_layers        = 3\n",
    "dropout           = True\n",
    "globlayer         = True\n",
    "freeze_layers_num = 400\n",
    "model_name        = 'EfficientNetB7_optimized'\n",
    "\n",
    "# Create numpy array and generators for the model\n",
    "X_train, y_train = SkinCancerCnn.create_image_arr(img_size, \n",
    "                                                  multiclass_directories[0], \n",
    "                                                  multi_categories,\n",
    "                                                  tf.keras.applications.efficientnet.preprocess_input,\n",
    "                                                  'train')\n",
    "\n",
    "train_multi_generator, val_multi_generator, multi_val_steps = my_skin_cnn_opt.create_multiclass_generators(X_train,\n",
    "                                                                                                           y_train,\n",
    "                                                                                                           multiclass_directories[0],\n",
    "                                                                                                           multiclass_directories[1],                                       \n",
    "                                                                                                           multi_sizes,\n",
    "                                                                                                           tf.keras.applications.efficientnet.preprocess_input)\n",
    "\n",
    "# Training process\n",
    "efficientnetb7_optimized_model, efficientnetb7_optimized_history = my_skin_cnn_opt.multi_class_model_train(train_multi_generator,\n",
    "                                                                                                           val_multi_generator,\n",
    "                                                                                                           multi_val_steps, \n",
    "                                                                                                           current_model,\n",
    "                                                                                                           units,\n",
    "                                                                                                           num_layers,\n",
    "                                                                                                           dropout,\n",
    "                                                                                                           globlayer,\n",
    "                                                                                                           freeze_layers_num,\n",
    "                                                                                                           model_name)\n",
    "\n",
    "# Plot the training process\n",
    "SkinCancerCnn.plot_model(efficientnetb7_optimized_history, 'plots/optimized/EfficientNetB7_optimized_plot.jpg')\n",
    "\n",
    "# Save the model\n",
    "efficientnetb7_optimized_model.save('./new_models/optimized/EfficientNetB7_optimized_224x224_batch64_0.00001lr.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40b976c",
   "metadata": {},
   "source": [
    "### Hyperparameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79d2366",
   "metadata": {},
   "source": [
    "This section was used for Bayesian hyperparameter optimization. This is a long process because it tried different model configurations. On average it took about 15 hours/model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85660a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for training \n",
    "lr                = 0.00001\n",
    "batch_size        = 64\n",
    "img_size          = 224\n",
    "epochs_num        = 13\n",
    "reduce_lr         = True\n",
    "early_stop        = True\n",
    "my_skin_cnn_hyper = SkinCancerCnn(img_size, lr, batch_size, reduce_lr, early_stop, epochs_num)\n",
    "\n",
    "\n",
    "# Create numpy array and generators for Multiclass\n",
    "X_train, y_train = SkinCancerCnn.create_image_arr(img_size,\n",
    "                                                  multiclass_directories[0],\n",
    "                                                  multi_categories, \n",
    "                                                  tf.keras.applications.efficientnet.preprocess_input, \n",
    "                                                  'train')\n",
    "\n",
    "# Multi class NN\n",
    "train_multi_generator, val_multi_generator, multi_val_steps = my_skin_cnn_hyper.create_multiclass_generators(X_train,\n",
    "                                                                                                             y_train,\n",
    "                                                                                                             multiclass_directories[0],\n",
    "                                                                                                             multiclass_directories[1],                                       \n",
    "                                                                                                             multi_sizes,\n",
    "                                                                                                             tf.keras.applications.efficientnet.preprocess_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6ef7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    \n",
    "    current_model     = EfficientNetB0\n",
    "    units             = hp.Int(\"units\", min_value = 128, max_value = 512, step = 128)\n",
    "    num_layers        = hp.Int(\"layers\", min_value = 1, max_value = 3)\n",
    "    dropout           = hp.Boolean(\"dropout\")\n",
    "    globlayer         = hp.Boolean(\"globlayer\")\n",
    "    freeze_layers_num = hp.Int(\"freeze_layers_num\", min_value = 0, max_value = 200, step = 50)\n",
    "    \n",
    "    # call existing model-building code with the hyperparameter values.\n",
    "    model = my_skin_cnn_hyper.multi_class_model(train_multi_generator,\n",
    "                                                val_multi_generator,\n",
    "                                                multi_val_steps,\n",
    "                                                current_model,\n",
    "                                                units,\n",
    "                                                num_layers,\n",
    "                                                dropout,\n",
    "                                                globlayer,\n",
    "                                                freeze_layers_num)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d262fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_model(kt.HyperParameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9e36d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.BayesianOptimization(build_model,\n",
    "                                max_trials = 15, \n",
    "                                # Do not resume the previous search in the same directory.\n",
    "                                overwrite = True,\n",
    "                                objective = kt.Objective(\"val_recall\", direction=  \"max\"),\n",
    "                                # Set a directory to store the intermediate results.\n",
    "                                directory = \"my_dir\")\n",
    "\n",
    "\n",
    "callbacks = []\n",
    "\n",
    "\n",
    "decrease_lr = ReduceLROnPlateau(monitor   = 'val_recall',\n",
    "                                factor    = 0.2,\n",
    "                                min_lr    = 0.0000001,\n",
    "                                patience  = 2,\n",
    "                                verbose   = 1,\n",
    "                                min_delta = 1e-6,\n",
    "                                mode      = 'max')\n",
    "\n",
    "early_stop = EarlyStopping(monitor              = 'val_recall',\n",
    "                           min_delta            = 0,\n",
    "                           patience             = 3,\n",
    "                           verbose              = 0,\n",
    "                           mode                 = 'max',\n",
    "                           baseline             = None,\n",
    "                           restore_best_weights = True)\n",
    "\n",
    "callbacks.append(decrease_lr)\n",
    "callbacks.append(early_stop)\n",
    "callbacks.append(keras.callbacks.TensorBoard(\"loggings_bayes_efficientnetb0\"))    \n",
    "\n",
    "tuner.search(train_multi_generator,\n",
    "             callbacks        = callbacks,\n",
    "             epochs           = 13, \n",
    "             validation_data  = val_multi_generator,\n",
    "             validation_steps = multi_val_steps)\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete.\n",
    "The optimal number of units is {best_hps.get('units')} \n",
    "and the optimal number of layers is {best_hps.get('layers')}\n",
    "and the optimal use of dropout is {best_hps.get('dropout')}\n",
    "and the optimal globlayer is {best_hps.get('globlayer')}\n",
    "and the optimal freeze_layers_num is {best_hps.get('freeze_layers_num')}.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaae1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized hyperparameters for NN\n",
    "lr                = 0.00001\n",
    "batch_size        = 64\n",
    "units             = best_hps.get('units')\n",
    "num_layers        = best_hps.get('layers')         \n",
    "dropout           = best_hps.get('dropout')\n",
    "globlayer         = best_hps.get('globlayer')\n",
    "freeze_layers_num = best_hps.get('freeze_layers_num')\n",
    "\n",
    "# Save the hyperparameters\n",
    "with open('./hyperparameters/hyperparameters_effnet.txt', 'w') as file:\n",
    "    file.write('lr ' + str(lr) + '\\n')\n",
    "    file.write('batch_size ' + str(batch_size) + '\\n')\n",
    "    file.write('units ' + str(units) + '\\n')\n",
    "    file.write('num_layers ' + str(num_layers) + '\\n')\n",
    "    file.write('dropout ' + str(dropout) + '\\n')\n",
    "    file.write('globlayer ' + str(globlayer) + '\\n')\n",
    "    file.write('freeze_layers_num ' + str(freeze_layers_num) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c13dd4d",
   "metadata": {},
   "source": [
    "### Loading the default models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8ae190",
   "metadata": {},
   "source": [
    "In this section, we are loading the trained default models for evaluation on the test set. We save the metrics scores, heatmaps with counter, and heatmaps with normalized values. The metrics we are interested in are the followings:\n",
    " - Accuracy\n",
    " - Precision\n",
    " - Recall / Balanced accuracy\n",
    " - F1-score\n",
    " - MCC\n",
    " - AVG\n",
    " \n",
    "We save the ROC-AUC curve for the best model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccec8a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d84251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG16\n",
    "model_vgg16 = keras.models.load_model('./models/default models/VGG16_224x224_batch64_frezze_full_original_0.0001lr.h5')\n",
    "\n",
    "X_test, y_true = SkinCancerCnn.create_image_arr(img_size, multi_test, multi_categories, tf.keras.applications.vgg16.preprocess_input, 'test') # create an image array with their corresponding labels for the test images\n",
    "y_pred = model_vgg16.predict(X_test)\n",
    "y_pred = [np.argmax(element) for element in y_pred]\n",
    "\n",
    "# Metrics, confusion matrix, normalized confusion matrix\n",
    "SkinCancerCnn.metrics_scores(y_true, y_pred, 'metrics/default/VGG16_default_metrics.txt')\n",
    "SkinCancerCnn.confusion_matrix(y_true, y_pred, None, 'heatmaps/default/VGG16_default_heatmap.jpg')\n",
    "SkinCancerCnn.confusion_matrix(y_true, y_pred, 'true', 'heatmaps/default/VGG16_default_heatmap_normalized.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d38c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# InceptionV3\n",
    "model_inceptionv3 = keras.models.load_model('./models/default models/InceptionV3_224x224_batch64_frezze_full_original_0.0001lr.h5')\n",
    "\n",
    "X_test, y_true = SkinCancerCnn.create_image_arr(img_size, multi_test, multi_categories, tf.keras.applications.inception_v3.preprocess_input, 'test') # create an image array with their corresponding labels for the test images\n",
    "y_pred = model_inceptionv3.predict(X_test)\n",
    "y_pred = [np.argmax(element) for element in y_pred]\n",
    "\n",
    "# Metrics, confusion matrix, normalized confusion matrix\n",
    "SkinCancerCnn.metrics_scores(y_true, y_pred, 'metrics/default/InceptionV3_default_metrics.txt')\n",
    "SkinCancerCnn.confusion_matrix(y_true, y_pred, None, 'heatmaps/default/InceptionV3_default_heatmap.jpg')\n",
    "SkinCancerCnn.confusion_matrix(y_true, y_pred, 'true', 'heatmaps/default/InceptionV3_default_heatmap_normalized.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4f050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet50\n",
    "model_resnet50 = keras.models.load_model('./models/default models/ResNet50_224x224_batch64_frezze_full_original_0.0001lr.h5')\n",
    "\n",
    "X_test, y_true = SkinCancerCnn.create_image_arr(img_size, multi_test, multi_categories, tf.keras.applications.resnet.preprocess_input, 'test') # create an image array with their corresponding labels for the test images\n",
    "y_p = model_resnet50.predict(X_test)\n",
    "y_pred = [np.argmax(element) for element in y_p]\n",
    "\n",
    "# Metrics, confusion matrix, normalized confusion matrix\n",
    "SkinCancerCnn.metrics_scores(y_true, y_pred, 'metrics/default/ResNet50_default_metrics.txt')\n",
    "SkinCancerCnn.confusion_matrix(y_true, y_pred, None, 'heatmaps/default/ResNet50_default_heatmap.jpg')\n",
    "SkinCancerCnn.confusion_matrix(y_true, y_pred, 'true', 'heatmaps/default/ResNet50_default_heatmap_normalized.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d272617d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet50 ROC-AUC curve\n",
    "SkinCancerCnn.plot_ROC_AUC_curve(y_true, y_p, 'ROC-AUC/default/ResNet50_default_ROC_AUC.jpg') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20521ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet152\n",
    "model_resnet152 = keras.models.load_model('./models/default models/ResNet152_224x224_batch64_frezze_full_original_0.0001lr.h5')\n",
    "\n",
    "X_test, y_true = SkinCancerCnn.create_image_arr(img_size, multi_test, multi_categories,  tf.keras.applications.resnet.preprocess_input, 'test') # create an image array with their corresponding labels for the test images\n",
    "y_pred = model_resnet152.predict(X_test)\n",
    "y_pred = [np.argmax(element) for element in y_pred]\n",
    "\n",
    "# Metrics, confusion matrix, normalized confusion matrix\n",
    "SkinCancerCnn.metrics_scores(y_true, y_pred, 'metrics/default/ResNet152_default_metrics.txt')\n",
    "SkinCancerCnn.confusion_matrix(y_true, y_pred, None, 'heatmaps/default/ResNet152_default_heatmap.jpg')\n",
    "SkinCancerCnn.confusion_matrix(y_true, y_pred, 'true', 'heatmaps/default/ResNet152_default_heatmap_normalized.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321f24fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DenseNet201\n",
    "model_densenet201 = keras.models.load_model('./models/default models/DenseNet201_224x224_batch64_frezze_full_original_0.0001lr.h5')\n",
    "\n",
    "X_test, y_true = SkinCancerCnn.create_image_arr(img_size, multi_test, multi_categories,  tf.keras.applications.densenet.preprocess_input, 'test') # create an image array with their corresponding labels for the test images\n",
    "y_pred = model_densenet201.predict(X_test)\n",
    "y_pred = [np.argmax(element) for element in y_pred]\n",
    "\n",
    "# Metrics, confusion matrix, normalized confusion matrix\n",
    "SkinCancerCnn.metrics_scores(y_true, y_pred, 'metrics/default/DenseNet201_default_metrics.txt')\n",
    "SkinCancerCnn.confusion_matrix(y_true, y_pred, None, 'heatmaps/default/DenseNet201_default_heatmap.jpg')\n",
    "SkinCancerCnn.confusion_matrix(y_true, y_pred, 'true', 'heatmaps/default/DenseNet201_default_heatmap_normalized.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77240885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MobileNetV2\n",
    "model_mobilenetv2 = keras.models.load_model('./models/default models/MobileNetV2_224x224_batch64_frezze_full_original_0.0001lr.h5')\n",
    "\n",
    "X_test, y_true = SkinCancerCnn.create_image_arr(img_size, multi_test, multi_categories,  tf.keras.applications.mobilenet_v2.preprocess_input,  'test') # create an image array with their corresponding labels for the test images\n",
    "y_pred = model_mobilenetv2.predict(X_test)\n",
    "y_pred = [np.argmax(element) for element in y_pred]\n",
    "\n",
    "# Metrics, confusion matrix, normalized confusion matrix\n",
    "SkinCancerCnn.metrics_scores(y_true, y_pred, 'metrics/default/MobileNetV2_default_metrics.txt')\n",
    "SkinCancerCnn.confusion_matrix(y_true, y_pred, None, 'heatmaps/default/MobileNetV2_default_heatmap.jpg')\n",
    "SkinCancerCnn.confusion_matrix(y_true, y_pred, 'true', 'heatmaps/default/MobileNetV2_default_heatmap_normalized.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e358c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EfficientNetB0\n",
    "model_efficientnetb0 = keras.models.load_model('./models/default models/EfficientNetB0_224x224_batch64_frezze_full_original_0.0001lr.h5')\n",
    "\n",
    "X_test, y_true = SkinCancerCnn.create_image_arr(img_size, multi_test, multi_categories,  tf.keras.applications.efficientnet.preprocess_input,  'test') # create an image array with their corresponding labels for the test images\n",
    "y_pred = model_efficientnetb0.predict(X_test)\n",
    "y_pred = [np.argmax(element) for element in y_pred]\n",
    "\n",
    "# Metrics, confusion matrix, normalized confusion matrix\n",
    "SkinCancerCnn.metrics_scores(y_true, y_pred, 'metrics/default/EfficientNetB0_default_metrics.txt')\n",
    "SkinCancerCnn.confusion_matrix(y_true, y_pred, None, 'heatmaps/default/EfficientNetB0_default_heatmap.jpg')\n",
    "SkinCancerCnn.confusion_matrix(y_true, y_pred, 'true', 'heatmaps/default/EfficientNetB0_default_heatmap_normalized.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec82fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EfficientNetB7\n",
    "model_efficientnetb7 = keras.models.load_model('./models/default models/EfficientNetB7_224x224_batch64_frezze_full_original_0.0001lr.h5')\n",
    "\n",
    "X_test, y_true = SkinCancerCnn.create_image_arr(img_size, multi_test, multi_categories,  tf.keras.applications.efficientnet.preprocess_input,  'test') # create an image array with their corresponding labels for the test images\n",
    "y_pred = model_efficientnetb7.predict(X_test)\n",
    "y_pred = [np.argmax(element) for element in y_pred]\n",
    "\n",
    "# Metrics, confusion matrix, normalized confusion matrix\n",
    "SkinCancerCnn.metrics_scores(y_true, y_pred, 'metrics/default/EfficientNetB7_default_metrics.txt')\n",
    "SkinCancerCnn.confusion_matrix(y_true, y_pred, None, 'heatmaps/default/EfficientNetB7_default_heatmap.jpg')\n",
    "SkinCancerCnn.confusion_matrix(y_true, y_pred, 'true', 'heatmaps/default/EfficientNetB7_default_heatmap_normalized.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe50844",
   "metadata": {},
   "source": [
    "### Loading the optimized models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54189677",
   "metadata": {},
   "source": [
    "The procedure in this section is the same as in the 'default' models case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6546f8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126c8cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG16_optimized\n",
    "model_vgg16_optimized = keras.models.load_model('./models/optimized models/VGG16_optimized_224x224_batch64_0.00001lr.h5')\n",
    "\n",
    "X_test, y_true = SkinCancerCnn.create_image_arr(img_size, multi_test, multi_categories,  tf.keras.applications.vgg16.preprocess_input,  'test') # create an image array with their corresponding labels for the test images\n",
    "y_pred = model_vgg16_optimized.predict(X_test)\n",
    "y_pred = [np.argmax(element) for element in y_pred]\n",
    "\n",
    "# Metrics, confusion matrix, normalized confusion matrix\n",
    "SkinCancerCnn.metrics_scores(y_true, y_pred, 'metrics/optimized/VGG16_optimized_metrics.txt')\n",
    "SkinCancerCnn.confusion_matrix(y_true, y_pred, None, 'heatmaps/optimized/VGG16_optimized_heatmap.jpg')\n",
    "SkinCancerCnn.confusion_matrix(y_true, y_pred, 'true', 'heatmaps/optimized/VGG16_optimized_heatmap_normalized.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edbb9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# InceptionV3_optimized\n",
    "model_inceptionv3_optimized = keras.models.load_model('./models/optimized models/InceptionV3_optimized_224x224_batch64_0.00001lr.h5')\n",
    "\n",
    "X_test, y_true = SkinCancerCnn.create_image_arr(img_size, multi_test, multi_categories,  tf.keras.applications.inception_v3.preprocess_input,  'test') # create an image array with their corresponding labels for the test images\n",
    "y_pred = model_inceptionv3_optimized.predict(X_test)\n",
    "y_pred = [np.argmax(element) for element in y_pred]\n",
    "\n",
    "# Metrics, confusion matrix, normalized confusion matrix\n",
    "SkinCancerCnn.metrics_scores(y_true, y_pred, 'metrics/optimized/InceptionV3_optimized_metrics.txt')\n",
    "SkinCancerCnn.confusion_matrix(y_true, y_pred, None, 'heatmaps/optimized/InceptionV3_optimized_heatmap.jpg')\n",
    "SkinCancerCnn.confusion_matrix(y_true, y_pred, 'true', 'heatmaps/optimized/InceptionV3_optimized_heatmap_normalized.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2d1cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet50_optimized\n",
    "model_resnet50_optimized = keras.models.load_model('./models/optimized models/ResNet50_optimized_224x224_batch64_0.00001lr.h5')\n",
    "\n",
    "X_test, y_true = SkinCancerCnn.create_image_arr(img_size, multi_test, multi_categories,  tf.keras.applications.resnet.preprocess_input,  'test') # create an image array with their corresponding labels for the test images\n",
    "y_pred = model_resnet50_optimized.predict(X_test)\n",
    "y_pred = [np.argmax(element) for element in y_pred]\n",
    "\n",
    "# Metrics, confusion matrix, normalized confusion matrix\n",
    "SkinCancerCnn.metrics_scores(y_true, y_pred, 'metrics/optimized/ResNet50_optimized_metrics.txt')\n",
    "SkinCancerCnn.confusion_matrix(y_true, y_pred, None, 'heatmaps/optimized/ResNet50_optimized_heatmap.jpg')\n",
    "SkinCancerCnn.confusion_matrix(y_true, y_pred, 'true', 'heatmaps/optimized/ResNet50_optimized_heatmap_normalized.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b08c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet152_optimized\n",
    "model_resnet152_optimized = keras.models.load_model('./models/optimized models/ResNet152_optimized_224x224_batch64_0.00001lr.h5')\n",
    "\n",
    "X_test, y_true = SkinCancerCnn.create_image_arr(img_size, multi_test, multi_categories,  tf.keras.applications.resnet.preprocess_input,  'test') # create an image array with their corresponding labels for the test images\n",
    "y_pred = model_resnet152_optimized.predict(X_test)\n",
    "y_pred = [np.argmax(element) for element in y_pred]\n",
    "\n",
    "# Metrics, confusion matrix, normalized confusion matrix\n",
    "SkinCancerCnn.metrics_scores(y_true, y_pred, 'metrics/optimized/ResNet152_optimized_metrics.txt')\n",
    "SkinCancerCnn.confusion_matrix(y_true, y_pred, None, 'heatmaps/optimized/ResNet152_optimized_heatmap.jpg')\n",
    "SkinCancerCnn.confusion_matrix(y_true, y_pred, 'true', 'heatmaps/optimized/ResNet152_optimized_heatmap_normalized.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf5e3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DenseNet201_optimized\n",
    "model_densenet201_optimized = keras.models.load_model('./models/optimized models/DenseNet201_optimized_224x224_batch64_0.00001lr.h5')\n",
    "\n",
    "X_test, y_true = SkinCancerCnn.create_image_arr(img_size, multi_test, multi_categories,  tf.keras.applications.densenet.preprocess_input,  'test') # create an image array with their corresponding labels for the test images\n",
    "y_pred = model_densenet201_optimized.predict(X_test)\n",
    "y_pred = [np.argmax(element) for element in y_pred]\n",
    "\n",
    "# Metrics, confusion matrix, normalized confusion matrix\n",
    "SkinCancerCnn.metrics_scores(y_true, y_pred, 'metrics/optimized/DenseNet201_optimizedmetrics.txt')\n",
    "SkinCancerCnn.confusion_matrix(y_true, y_pred, None, 'heatmaps/optimized/DenseNet201_optimized_heatmap.jpg')\n",
    "SkinCancerCnn.confusion_matrix(y_true, y_pred, 'true', 'heatmaps/optimized/DenseNet201_optimized_heatmap_normalized.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca24a575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MobileNetV2_optimized\n",
    "model_mobilenetv2_optimized = keras.models.load_model('./models/optimized models/MobileNetV2_optimized_224x224_batch64_0.00001lr.h5')\n",
    "\n",
    "X_test, y_true = SkinCancerCnn.create_image_arr(img_size, multi_test, multi_categories,  tf.keras.applications.mobilenet_v2.preprocess_input,  'test') # create an image array with their corresponding labels for the test images\n",
    "y_pred = model_mobilenetv2_optimized.predict(X_test)\n",
    "y_pred = [np.argmax(element) for element in y_pred]\n",
    "\n",
    "# Metrics, confusion matrix, normalized confusion matrix\n",
    "SkinCancerCnn.metrics_scores(y_true, y_pred, 'metrics/optimized/MobileNetV2_optimized_metrics.txt')\n",
    "SkinCancerCnn.confusion_matrix(y_true, y_pred, None, 'heatmaps/optimized/MobileNetV2_optimized_heatmap.jpg')\n",
    "SkinCancerCnn.confusion_matrix(y_true, y_pred, 'true', 'heatmaps/optimized/MobileNetV2_optimized_heatmap_normalized.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bfeabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EfficientNetB0_optimized\n",
    "model_efficientnetb0_optimized = keras.models.load_model('./models/optimized models/EfficientNetB0_optimized_224x224_batch64_0.00001lr.h5')\n",
    "\n",
    "X_test, y_true = SkinCancerCnn.create_image_arr(img_size, multi_test, multi_categories,  tf.keras.applications.efficientnet.preprocess_input,  'test') # create an image array with their corresponding labels for the test images\n",
    "y_pred = model_efficientnetb0_optimized.predict(X_test)\n",
    "y_pred = [np.argmax(element) for element in y_pred]\n",
    "\n",
    "# Metrics, confusion matrix, normalized confusion matrix\n",
    "SkinCancerCnn.metrics_scores(y_true, y_pred, 'metrics/optimized/EfficientNetB0_optimized_metrics.txt')\n",
    "SkinCancerCnn.confusion_matrix(y_true, y_pred, None, 'heatmaps/optimized/EfficientNetB0_optimized_heatmap.jpg')\n",
    "SkinCancerCnn.confusion_matrix(y_true, y_pred, 'true', 'heatmaps/optimized/EfficientNetB0_optimized_heatmap_normalized.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234f28b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EfficientNetB7_optimized\n",
    "model_efficientnetb7_optimized = keras.models.load_model('./models/optimized models/EfficientNetB7_optimized_224x224_batch64_0.00001lr.h5')\n",
    "\n",
    "X_test, y_true = SkinCancerCnn.create_image_arr(img_size, multi_test, multi_categories,  tf.keras.applications.efficientnet.preprocess_input,  'test') # create an image array with their corresponding labels for the test images\n",
    "y_p = model_efficientnetb7_optimized.predict(X_test)\n",
    "y_pred = [np.argmax(element) for element in y_p]\n",
    "\n",
    "# Metrics, confusion matrix, normalized confusion matrix\n",
    "SkinCancerCnn.metrics_scores(y_true, y_pred, 'metrics/optimized/EfficientNetB7_optimized_metrics.txt')\n",
    "SkinCancerCnn.confusion_matrix(y_true, y_pred, None, 'heatmaps/optimized/EfficientNetB7_optimized_heatmap.jpg')\n",
    "SkinCancerCnn.confusion_matrix(y_true, y_pred, 'true', 'heatmaps/optimized/EfficientNetB7_optimized_heatmap_normalized.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac8caf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EfficientNetB7_optimized ROC-AUC curve\n",
    "SkinCancerCnn.plot_ROC_AUC_curve(y_true, y_p, 'ROC-AUC/optimized/EfficientNetB7_optimized_ROC_AUC.jpg') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8c9af6",
   "metadata": {},
   "source": [
    "### Weighted average ensemble model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348b150e",
   "metadata": {},
   "source": [
    "In this section, we took the 3 best models and use them for prediction. After the prediction phase we multiple that with the best weights for the ensemble predictions. In the last step, we save the metrics, confusion matrices, and ROC-AUC curve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da3c735",
   "metadata": {},
   "source": [
    "#### Grid search for optimal weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe9c8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search for weights\n",
    "\n",
    "def best_weights_grid_search(preds):\n",
    "    max_avg = 0.0 \n",
    "\n",
    "    w1_opt = 0.0\n",
    "    w2_opt = 0.0\n",
    "    w3_opt = 0.0\n",
    "\n",
    "    for w1 in range(0,9):\n",
    "        for w2 in range(0,9):\n",
    "            for w3 in range(0,9):\n",
    "\n",
    "                wts                = [w1/10., w2/10., w3/10.]\n",
    "                wted_preds1        = np.tensordot(preds, wts, axes=((0),(0)))\n",
    "                wted_ensemble_pred = np.argmax(wted_preds1, axis=1)\n",
    "                accuracy           = accuracy_score(y_true, wted_ensemble_pred) # How many we got right\n",
    "                recall             = recall_score(y_true, wted_ensemble_pred, average = 'macro') # Truth labels are the baseline, All class[i] truth how many we got right\n",
    "                precision          = precision_score(y_true, wted_ensemble_pred, average = 'macro') # All class[i] predictions how many we got right\n",
    "                f1_score           = 2 * (precision * recall) / (precision + recall)\n",
    "                balanced_accuracy  = balanced_accuracy_score(y_true, wted_ensemble_pred)\n",
    "                mcc                = matthews_corrcoef(y_true, wted_ensemble_pred)\n",
    "\n",
    "                AVG = (balanced_accuracy + f1_score + mcc) / 3.0\n",
    "\n",
    "                if AVG > max_avg:\n",
    "                    max_avg = AVG\n",
    "                    w1_opt = w1 / 10.0\n",
    "                    w2_opt = w2 / 10.0\n",
    "                    w3_opt = w3 / 10.0\n",
    "\n",
    "    print('Best weights: ' + str(w1_opt) + ' ' + str(w2_opt) + ' ' + str(w3_opt) + ' with AVG: ' + str(max_avg)) \n",
    "    return [w1_opt, w2_opt, w3_opt]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb30112",
   "metadata": {},
   "source": [
    "Loading the three models and make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96f3893",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0321a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_efficientnetb7_ensemble = keras.models.load_model('./models/optimized models/EfficientNetB7_optimized_224x224_batch64_0.00001lr.h5')\n",
    "model_resnet50_ensemble       = keras.models.load_model('./models/optimized models/ResNet50_optimized_224x224_batch64_0.00001lr.h5')\n",
    "model_inceptionv3_ensemble    = keras.models.load_model('./models/optimized models/InceptionV3_optimized_224x224_batch64_0.00001lr.h5')\n",
    "\n",
    "X_test, y_true = SkinCancerCnn.create_image_arr(img_size, multi_test, multi_categories,  tf.keras.applications.efficientnet.preprocess_input,  'test') # create an image array with their corresponding labels for the test images\n",
    "X_test2,_      = SkinCancerCnn.create_image_arr(img_size, multi_test, multi_categories,  tf.keras.applications.resnet.preprocess_input,  'test') # create an image array with their corresponding labels for the test images\n",
    "X_test3,_      = SkinCancerCnn.create_image_arr(img_size, multi_test, multi_categories,  tf.keras.applications.inception_v3.preprocess_input,  'test') # create an image array with their corresponding labels for the test images\n",
    "\n",
    "\n",
    "pred_efficientnet = model_efficientnetb7_ensemble.predict(X_test)\n",
    "pred_resnet       = model_resnet50_ensemble.predict(X_test2)\n",
    "pred_inception    = model_inceptionv3_ensemble.predict(X_test3)\n",
    "preds = [pred_efficientnet, pred_resnet, pred_inception]\n",
    "preds = np.array(preds)\n",
    "\n",
    "# Weighted\n",
    "#weights = [0.5, 0.7, 0.4]\n",
    "weights = best_weights_grid_search(preds)\n",
    "\n",
    "# Use tensordot to sum the products of all elements over specified axes.\n",
    "weighted_preds = np.tensordot(preds, weights, axes=((0),(0)))\n",
    "weighted_ensemble_prediction = np.argmax(weighted_preds, axis=1)\n",
    "\n",
    "# Metrics, confusion matrix, normalized confusion matrix\n",
    "SkinCancerCnn.metrics_scores(y_true, weighted_ensemble_prediction, 'metrics/ensemble/ensemble_metrics.txt')\n",
    "SkinCancerCnn.confusion_matrix(y_true, weighted_ensemble_prediction, 'true', 'heatmaps/ensemble/ensemble_heatmap.jpg')\n",
    "SkinCancerCnn.confusion_matrix(y_true, weighted_ensemble_prediction, None, 'heatmaps/ensemble/ensemble_heatmap_normalized.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a8d07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC-AUC curve\n",
    "SkinCancerCnn.plot_ROC_AUC_curve(y_true, weighted_preds, 'ROC-AUC/ensemble/ensemble_ROC_AUC.jpg') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c701df",
   "metadata": {},
   "source": [
    "### Plots for metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4c7b09",
   "metadata": {},
   "source": [
    "In this section, I created the plots for the default, optimized, and the ensemble model. I created one plot for the accuracies and one for the AVG metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8c1145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_visualization(models, values, fname, red):\n",
    "\n",
    "    plt.rcParams[\"figure.figsize\"] = (13,7)\n",
    "\n",
    "    x  = np.arange(len(models))\n",
    "    y  = values\n",
    "\n",
    "    width   = 0.7\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    if red:\n",
    "        pps = ax.bar(x, y, width, align='center', color = (0.8, 0.1, 0.1, 0.8))\n",
    "    else:\n",
    "        pps = ax.bar(x, y, width, align='center')\n",
    "        \n",
    "    plt.xticks(x, models)\n",
    "\n",
    "    for p in pps:\n",
    "        height = p.get_height()\n",
    "        ax.text(x=p.get_x() + p.get_width() / 2, y=height - 9,\n",
    "          s=\"{}%\".format(height),\n",
    "          ha='center', fontsize=14, color = 'white')\n",
    "\n",
    "    plt.show()\n",
    "    fig.savefig(fname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5a65f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_default_acc = ['DenseNet201', 'ResNet50', 'EfficientNetB0', 'ResNet152', 'EfficientNetB7', 'VGG16', 'InceptionV3', 'MobileNetV2']\n",
    "default_acc        = [77.53, 77.53, 74.34, 73.80, 73.14, 72.47, 70.21, 67.02]\n",
    "models_default_avg = ['ResNet50', 'ResNet152', 'DenseNet201', 'EfficientNetB7', 'EfficientNetB0', 'VGG16', 'MobileNetV2', 'InceptionV3']\n",
    "default_avg        = [65.90, 65.53, 63.59, 61.50, 62.08, 58.79, 56.76, 55.48]\n",
    "\n",
    "models_optimized_acc = ['ResNet50', 'EfficientNetB7',  'InceptionV3', 'EfficientNetB0', 'ResNet152', 'VGG16','DenseNet201', 'MobileNetV2']\n",
    "optimized_acc        = [86.30, 85.77, 85.77, 84.57, 83.78, 83.78, 81.25, 76.06]\n",
    "models_optimized_avg = ['EfficientNetB7', 'ResNet50', 'InceptionV3', 'ResNet152', 'EfficientNetB0', 'DenseNet201', 'VGG16', 'MobileNetV2']\n",
    "optimized_avg        = [78.95, 78.88, 77.20, 76.14, 75.13, 73.01, 72.54, 63.88]\n",
    "\n",
    "models_ensemble_avg = ['eERI', 'EfficientNetB7', 'ResNet50', 'InceptionV3', 'ResNet152', 'EfficientNetB0', 'DenseNet201', 'VGG16', 'MobileNetV2']\n",
    "ensemble_avg        = [84.49, 78.95, 78.88, 77.20, 76.14, 75.13, 73.01, 72.54, 63.88]\n",
    "\n",
    "metrics_visualization(models_default_acc, default_acc, 'metrics_visualization/default/default_acc.jpg', True)\n",
    "metrics_visualization(models_default_avg, default_avg, 'metrics_visualization/default/default_avg.jpg', False)\n",
    "metrics_visualization(models_optimized_acc, optimized_acc, 'metrics_visualization/optimized/optimized_acc.jpg', True)\n",
    "metrics_visualization(models_optimized_avg, optimized_avg, 'metrics_visualization/optimized/optimized_avg.jpg', False)\n",
    "metrics_visualization(models_ensemble_avg, ensemble_avg, 'metrics_visualization/ensemble/ensemble_avg.jpg', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d1da83",
   "metadata": {},
   "source": [
    "### Gaussian and Poisson noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750a787e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 224"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f59fb2",
   "metadata": {},
   "source": [
    "I created new folders (new test set) for the Poisson and Gaussian noise images. After that, I tested the best 'eERI' model with these images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0021031f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_perturbated_folders(path, mode):\n",
    "\n",
    "    directories = ['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc'] \n",
    "    images_path = []\n",
    "\n",
    "    for directory in directories:\n",
    "        for img in os.listdir(path + '/' + directory):\n",
    "            images_path.append(os.path.join(path,directory,img))\n",
    "\n",
    "\n",
    "    for img_path in images_path:\n",
    "        img = skimage.io.imread(img_path)\n",
    "        gimg = skimage.util.random_noise(img, mode=mode)\n",
    "        skimage.io.imsave(img_path, gimg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076ea17a",
   "metadata": {},
   "source": [
    "These folders are already there!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd553ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''poisson_path  = 'D:\\\\MSC_szakdolgozat\\\\MSC_szakdoga\\\\Datas\\\\Multiclass\\\\Poisson test set'\n",
    "gaussian_path = 'D:\\\\MSC_szakdolgozat\\\\MSC_szakdoga\\\\Datas\\\\Multiclass\\\\Gaussian test set'\n",
    "\n",
    "create_perturbated_folders(poisson_path, 'poisson')\n",
    "create_perturbated_folders(gaussian_path, 'gaussian')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7e2f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial_black_box_perturbation(path, perturbation, mode):\n",
    "\n",
    "    model_efficientnetb7 = keras.models.load_model('./models/optimized models/EfficientNetB7_optimized_224x224_batch64_0.00001lr.h5')\n",
    "    model_resnet50       = keras.models.load_model('./models/optimized models/ResNet50_optimized_224x224_batch64_0.00001lr.h5')\n",
    "    model_inceptionv3    = keras.models.load_model('./models/optimized models/InceptionV3_optimized_224x224_batch64_0.00001lr.h5')\n",
    "\n",
    "    multi_test = './Datas/Multiclass/' + path\n",
    "\n",
    "    X_test, y_true = SkinCancerCnn.create_image_arr(img_size, multi_test, multi_categories,  tf.keras.applications.efficientnet.preprocess_input,  'test') # create an image array with their corresponding labels for the test images\n",
    "    X_test2,_      = SkinCancerCnn.create_image_arr(img_size, multi_test, multi_categories,  tf.keras.applications.resnet.preprocess_input,  'test') # create an image array with their corresponding labels for the test images\n",
    "    X_test3,_      = SkinCancerCnn.create_image_arr(img_size, multi_test, multi_categories,  tf.keras.applications.inception_v3.preprocess_input,  'test') # create an image array with their corresponding labels for the test images\n",
    "\n",
    "    pred_efficientnet = model_efficientnetb7.predict(X_test)\n",
    "    pred_resnet       = model_resnet50.predict(X_test2)\n",
    "    pred_inception    = model_inceptionv3.predict(X_test3)\n",
    "    preds = [pred_efficientnet, pred_resnet, pred_inception]\n",
    "    preds = np.array(preds)\n",
    "\n",
    "    #weighted average\n",
    "    weights = [0.5, 0.7, 0.4]\n",
    "\n",
    "    #Use tensordot to sum the products of all elements over specified axes.\n",
    "    weighted_preds               = np.tensordot(preds, weights, axes=((0),(0)))\n",
    "    weighted_ensemble_prediction = np.argmax(weighted_preds, axis=1)\n",
    "\n",
    "    SkinCancerCnn.metrics_scores(y_true, weighted_ensemble_prediction, 'metrics/' + mode + '/' + perturbation + '.txt')\n",
    "    SkinCancerCnn.confusion_matrix(y_true, weighted_ensemble_prediction, 'true', 'heatmaps/perturbation/' + perturbation + '.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8f6fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "adversarial_black_box_perturbation('Poisson test set', 'Possion_perturbation_eERI', 'poisson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c8b12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "adversarial_black_box_perturbation('Gaussian test set', 'Gaussian_perturbation_eERI', 'gauss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f6afe6",
   "metadata": {},
   "source": [
    "This code section was used for CNN confidence level check with Poisson and Gaussian noise images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9cb2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence_visualization(model_origin, model_poisson, model_gauss, fname):\n",
    "    \n",
    "    plt.rcParams[\"figure.figsize\"] = (12,8)\n",
    "    labels = [ 'akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc']\n",
    "\n",
    "    x       = np.arange(len(labels))  # the label locations\n",
    "    width   = 0.28  # the width of the bars\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    bar1 = ax.bar(x, model_origin, width, color = 'r', label = 'eERI')\n",
    "    bar2 = ax.bar(x+width, model_poisson, width, color='g', label = 'eERI Poisson')\n",
    "    bar3 = ax.bar(x+width*2, model_gauss, width, color = 'b', label = 'eERI Gaussian')\n",
    "\n",
    "\n",
    "    # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "    ax.set_xticks(x, labels)\n",
    "    ax.legend()\n",
    "\n",
    "    ax.bar_label(bar1, padding=3)\n",
    "    ax.bar_label(bar2, padding=3)\n",
    "    ax.bar_label(bar3, padding=3)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(fname)\n",
    "\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641d2653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This values were the confidence levels for the given images (for more information see the .pdf file)\n",
    "model_origin  = [0.03, 0.08, 0.17, 0.05, 87.27, 12.36, 0.05]\n",
    "model_poisson = [0.01, 0.04, 0.19, 0.08, 60.54, 39.09, 0.05]\n",
    "model_gauss   = [0.01, 0.03, 0.71, 0.08, 14.96, 84.15, 0.07]\n",
    "\n",
    "confidence_visualization(model_origin, model_poisson, model_gauss, 'confidence.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3be5c6",
   "metadata": {},
   "source": [
    "### FGSM white box untargeted attack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84edde2",
   "metadata": {},
   "source": [
    "I tested the optimized EfficientNetB7 and ResNet50 models with white box untargeted FGSM attacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d89dd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_efficientnetb7 = keras.models.load_model('./models/optimized models/EfficientNetB7_optimized_224x224_batch64_0.00001lr.h5')\n",
    "model_resnet50       = keras.models.load_model('./models/optimized models/ResNet50_optimized_224x224_batch64_0.00001lr.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1c3427",
   "metadata": {},
   "source": [
    "#### Helper functions for FGSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c00806",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "def preprocess(image, preprocess, img_size):\n",
    "    \n",
    "    image = tf.image.resize(image, (img_size, img_size))\n",
    "    image = preprocess(image)\n",
    "    image = image[None, ...]\n",
    "    return image\n",
    "\n",
    "def create_adversarial_pattern(input_image, input_label, model):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(input_image)\n",
    "        prediction = model(input_image)\n",
    "        loss = loss_object(input_label, prediction)\n",
    "\n",
    "    # Get the gradients of the loss w.r.t to the input image.\n",
    "    gradient = tape.gradient(loss, input_image)\n",
    "    # Get the sign of the gradients to create the perturbation\n",
    "    signed_grad = tf.sign(gradient)\n",
    "    return signed_grad\n",
    "\n",
    "def mapping_labels(index):\n",
    "    \n",
    "    labels_dict={\n",
    "      0: 'akiec', \n",
    "      1: 'bcc', \n",
    "      2: 'bkl', \n",
    "      3: 'df', \n",
    "      4: 'mel',\n",
    "      5: 'nv', \n",
    "      6: 'vasc' \n",
    "    } \n",
    "    \n",
    "    return labels_dict[index]\n",
    "\n",
    "def mapping_indices(index):\n",
    "    \n",
    "    labels_dict={\n",
    "      'akiec' : 0, \n",
    "      'bcc': 1, \n",
    "      'bkl': 2, \n",
    "      'df': 3, \n",
    "      'mel': 4, \n",
    "      'nv': 5, \n",
    "      'vasc': 6 \n",
    "    } \n",
    "    \n",
    "    return labels_dict[index]\n",
    "\n",
    "def display_images(image, model):\n",
    "    \n",
    "    predictions = model.predict(image)\n",
    "    label = np.argmax(predictions, axis = 1)\n",
    "    confidence = float(max((model.predict(image))[0]))\n",
    "    #plt.figure()\n",
    "    #plt.imshow(image[0]  / 255.0) #*0.5+0.5\n",
    "    #plt.imshow(image[0] *0.5+0.5) #*0.5+0.5\n",
    "    #print(label)\n",
    "    #plt.title('\\n {} : {:.2f}% Confidence'.format(mapping_labels(label[0]), confidence*100))\n",
    "    #plt.show()\n",
    "    return label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85985944",
   "metadata": {},
   "source": [
    "Reading images from the original Test set, and adding perturbation for the images one by one iteratively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629695b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FGSM(img_size, model, prep, epsilons):\n",
    "\n",
    "    path        = './Datas/Multiclass/Test set'\n",
    "    directories = ['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc'] \n",
    "    y_true      = []\n",
    "    y_pred      = []\n",
    "    ind         = 0\n",
    "\n",
    "    for directory in directories:\n",
    "        for img in os.listdir(path + '/' + directory):\n",
    "\n",
    "            img_path  = os.path.join(path,directory,img)\n",
    "            image_raw = tf.io.read_file(img_path)\n",
    "            image     = tf.image.decode_image(image_raw)\n",
    "\n",
    "            image       = preprocess(image, prep, img_size)\n",
    "            image_probs = model.predict(image)\n",
    "            label       = np.argmax(image_probs, axis = 1)\n",
    "\n",
    "            skin_index = mapping_indices(directory)\n",
    "            y_true.append(skin_index)\n",
    "\n",
    "            label      = tf.one_hot(skin_index, image_probs.shape[-1])\n",
    "            label      = tf.reshape(label, (1, image_probs.shape[-1]))\n",
    "\n",
    "            perturbations = create_adversarial_pattern(image, label, model)\n",
    "\n",
    "            epsilons = epsilons\n",
    "\n",
    "            for i, eps in enumerate(epsilons):\n",
    "                adv_x = image + eps*perturbations            \n",
    "                pred_label = display_images(adv_x, model)\n",
    "                y_pred.append(pred_label[0])\n",
    "            ind += 1\n",
    "            print('The current image is: ' + str(ind))\n",
    "            \n",
    "    return y_true, y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b372d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_true, y_pred = FGSM(224, model_efficientnetb7, tf.keras.applications.efficientnet.preprocess_input, [0.1])\n",
    "#y_true, y_pred = FGSM(224, model_efficientnetb7, tf.keras.applications.efficientnet.preprocess_input, [0.3])\n",
    "#y_true, y_pred = FGSM(224, model_efficientnetb7, tf.keras.applications.efficientnet.preprocess_input, [0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806a567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred = FGSM(224, model_resnet50, tf.keras.applications.resnet.preprocess_input, [0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e24bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_t = np.array(y_true)\n",
    "y_p = np.array(y_pred)\n",
    "\n",
    "SkinCancerCnn.metrics_scores(y_true, y_pred, 'metrics/FGSM/ResNet50_fgsm_metrics_0_5.txt')\n",
    "SkinCancerCnn.confusion_matrix(y_true, y_pred, 'true', 'heatmaps/perturbation/ResNet50_fgsm_conf_mat_0_5.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
